{"cells":[{"cell_type":"markdown","id":"1500e7f3-bd3b-4ea3-b073-fe4e08b4f1ff","metadata":{"id":"1500e7f3-bd3b-4ea3-b073-fe4e08b4f1ff"},"source":["### 6개키워드 중 하나만 나오게 하는 샘플모델링"]},{"cell_type":"markdown","id":"cb648cb2-9c39-41e4-903d-54e79767a6c0","metadata":{"id":"cb648cb2-9c39-41e4-903d-54e79767a6c0"},"source":["## ./crawled_img/(분위기키워드 폴더명)\n"," - joyful(활발한)\n"," - adventure(모험적인)\n"," - cultural(문화적인)\n"," - tradition(전통적인)\n"," - nature(자연)\n"," - art(예술적인)"]},{"cell_type":"markdown","id":"99ac69d7-7abd-462e-8a22-8ca7b60ce628","metadata":{"id":"99ac69d7-7abd-462e-8a22-8ca7b60ce628"},"source":["### CNN 사용한 샘플모델링(데이터 증강X) - 200장"]},{"cell_type":"code","execution_count":null,"id":"1b16194f-fe91-47f6-bbc3-b15c5274d046","metadata":{"id":"1b16194f-fe91-47f6-bbc3-b15c5274d046","outputId":"d0beb295-14fa-40ed-8f7b-70d8ad48e917"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","30/30 [==============================] - 26s 828ms/step - loss: 1.7257 - accuracy: 0.2760 - val_loss: 1.6282 - val_accuracy: 0.3417\n","Epoch 2/20\n","30/30 [==============================] - 26s 867ms/step - loss: 1.3796 - accuracy: 0.4396 - val_loss: 1.2716 - val_accuracy: 0.4750\n","Epoch 3/20\n","30/30 [==============================] - 28s 953ms/step - loss: 1.0722 - accuracy: 0.5865 - val_loss: 1.2062 - val_accuracy: 0.5167\n","Epoch 4/20\n","30/30 [==============================] - 34s 1s/step - loss: 0.8515 - accuracy: 0.6792 - val_loss: 1.3690 - val_accuracy: 0.5000\n","Epoch 5/20\n","30/30 [==============================] - 47s 2s/step - loss: 0.5925 - accuracy: 0.7990 - val_loss: 1.3658 - val_accuracy: 0.4833\n","Epoch 6/20\n","30/30 [==============================] - 50s 2s/step - loss: 0.3522 - accuracy: 0.8771 - val_loss: 1.3723 - val_accuracy: 0.5792\n","Epoch 7/20\n","30/30 [==============================] - 50s 2s/step - loss: 0.1655 - accuracy: 0.9479 - val_loss: 1.2929 - val_accuracy: 0.6083\n","Epoch 8/20\n","30/30 [==============================] - 50s 2s/step - loss: 0.1000 - accuracy: 0.9781 - val_loss: 1.5182 - val_accuracy: 0.5958\n","Epoch 9/20\n","30/30 [==============================] - 57s 2s/step - loss: 0.0263 - accuracy: 0.9969 - val_loss: 1.8314 - val_accuracy: 0.5958\n","Epoch 10/20\n","30/30 [==============================] - 45s 2s/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 1.9913 - val_accuracy: 0.6042\n","Epoch 11/20\n","30/30 [==============================] - 51s 2s/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 2.0846 - val_accuracy: 0.6042\n","Epoch 12/20\n","30/30 [==============================] - 45s 1s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 2.2371 - val_accuracy: 0.6083\n","Epoch 13/20\n","30/30 [==============================] - 55s 2s/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 2.3391 - val_accuracy: 0.6333\n","Epoch 14/20\n","30/30 [==============================] - 48s 2s/step - loss: 8.1523e-04 - accuracy: 1.0000 - val_loss: 2.3789 - val_accuracy: 0.5958\n","Epoch 15/20\n","30/30 [==============================] - 51s 2s/step - loss: 5.0372e-04 - accuracy: 1.0000 - val_loss: 2.4363 - val_accuracy: 0.5958\n","Epoch 16/20\n","30/30 [==============================] - 46s 2s/step - loss: 3.9408e-04 - accuracy: 1.0000 - val_loss: 2.4927 - val_accuracy: 0.5917\n","Epoch 17/20\n","30/30 [==============================] - 45s 2s/step - loss: 3.1946e-04 - accuracy: 1.0000 - val_loss: 2.5056 - val_accuracy: 0.5917\n","Epoch 18/20\n","30/30 [==============================] - 47s 2s/step - loss: 2.7188e-04 - accuracy: 1.0000 - val_loss: 2.5661 - val_accuracy: 0.5875\n","Epoch 19/20\n","30/30 [==============================] - 46s 2s/step - loss: 2.1699e-04 - accuracy: 1.0000 - val_loss: 2.6004 - val_accuracy: 0.5792\n","Epoch 20/20\n","30/30 [==============================] - 47s 2s/step - loss: 1.7565e-04 - accuracy: 1.0000 - val_loss: 2.6328 - val_accuracy: 0.5875\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x20d4aca4b20>"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","\n","# 이미지 크기 설정\n","img_size = (224, 224)\n","\n","# 데이터 로드 및 전처리 함수\n","def load_and_preprocess_data(image_paths, label, brightness=True):\n","    data = []\n","    labels = []\n","\n","    for img_path in image_paths:\n","        img = load_img(img_path, target_size=img_size)\n","        img_array = img_to_array(img)\n","\n","        # 이미지 명도와 채도 높이기\n","        if brightness:\n","            img_array = cv2.convertScaleAbs(img_array, alpha=1.2, beta=30)\n","\n","        img_array = img_array.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n","\n","        data.append(img_array)\n","        labels.append(label)\n","\n","    return np.array(data), np.array(labels)\n","\n","# 카테고리 리스트\n","categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n","num_classes = len(categories)\n","\n","# 이미지 데이터 로드 및 전처리\n","all_data = []\n","all_labels = []\n","\n","for i, category in enumerate(categories):\n","    category_path = os.path.join(\"crawled_img\", category)\n","    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)[:200]]  # 처음 200개 이미지만 선택\n","\n","    data, labels = load_and_preprocess_data(category_images, label=i)\n","    all_data.extend(data)\n","    all_labels.extend(labels)\n","\n","# 데이터 합치기 및 레이블 변환\n","X = np.array(all_data)\n","y = to_categorical(np.array(all_labels), num_classes=num_classes)\n","\n","# 학습 및 테스트 데이터로 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# CNN 모델 생성\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 학습\n","model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"]},{"cell_type":"markdown","id":"706ac77e-9179-4a76-9bcf-7319f8bbaaf8","metadata":{"id":"706ac77e-9179-4a76-9bcf-7319f8bbaaf8"},"source":["#### 결과\n","- **훈련 데이터는 높은 정확도를 보임, 하지만 검증 데이터는 정확도가 낮은 양상을 보임**\n","- **즉, 과적합이 발생**\n","    - 모델이 훈련데이터에 맞춰져있어 새로운 데이터를 일반화시키지 못함\n","- **epoch = 20, batch_size = 32, test_size=0.2**\n","- **코드 수정 방안**\n","    - 데이터 증강\n","        - 증강을 너무 많이 쓰면 훈련속도 느려지고, 과도한 증강추가는 효과적이지 않을수있음\n","    - 모델 복잡도 조정\n","    - dropout\n","    - 하이퍼파라미터 조정\n","    - 에포크 수 조정\n","    - 더 많은 데이터로 학습시키기\n","    - 다양한 최적화 알고리즘 사용 : SGD, RMSprop\n","    - 데이터 제너레이터"]},{"cell_type":"markdown","id":"b8094c87-a9a6-4f1a-9cba-a1b31c4ca5b6","metadata":{"id":"b8094c87-a9a6-4f1a-9cba-a1b31c4ca5b6"},"source":["## _______________________________________________________"]},{"cell_type":"markdown","id":"5f94b1b8-0c18-4320-a932-16d3f682a68c","metadata":{"id":"5f94b1b8-0c18-4320-a932-16d3f682a68c"},"source":["### CNN 사용한 샘플모델링(데이터 증강) - 이미지 200장"]},{"cell_type":"code","execution_count":null,"id":"e2a0a2f4-8aab-42c3-afd8-3a8656d679c0","metadata":{"scrolled":true,"id":"e2a0a2f4-8aab-42c3-afd8-3a8656d679c0","outputId":"5df0c33a-5e4f-49cb-9a3d-d1bf9fe131a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","30/30 [==============================] - 30s 944ms/step - loss: 2.0864 - accuracy: 0.1677 - val_loss: 1.7518 - val_accuracy: 0.1958\n","Epoch 2/20\n","30/30 [==============================] - 30s 997ms/step - loss: 1.7463 - accuracy: 0.2469 - val_loss: 1.6693 - val_accuracy: 0.2875\n","Epoch 3/20\n","30/30 [==============================] - 40s 1s/step - loss: 1.6676 - accuracy: 0.2958 - val_loss: 1.5040 - val_accuracy: 0.3542\n","Epoch 4/20\n","30/30 [==============================] - 48s 2s/step - loss: 1.5474 - accuracy: 0.3552 - val_loss: 2.0581 - val_accuracy: 0.2417\n","Epoch 5/20\n","30/30 [==============================] - 52s 2s/step - loss: 1.4991 - accuracy: 0.3531 - val_loss: 1.6579 - val_accuracy: 0.3625\n","Epoch 6/20\n","30/30 [==============================] - 52s 2s/step - loss: 1.4011 - accuracy: 0.4094 - val_loss: 1.9326 - val_accuracy: 0.2292\n","Epoch 7/20\n","30/30 [==============================] - 51s 2s/step - loss: 1.3927 - accuracy: 0.4313 - val_loss: 1.5280 - val_accuracy: 0.4083\n","Epoch 8/20\n","30/30 [==============================] - 55s 2s/step - loss: 1.3105 - accuracy: 0.4823 - val_loss: 1.1918 - val_accuracy: 0.5208\n","Epoch 9/20\n","30/30 [==============================] - 51s 2s/step - loss: 1.2979 - accuracy: 0.4635 - val_loss: 1.1032 - val_accuracy: 0.5708\n","Epoch 10/20\n","30/30 [==============================] - 56s 2s/step - loss: 1.2065 - accuracy: 0.5260 - val_loss: 1.1926 - val_accuracy: 0.5667\n","Epoch 11/20\n","30/30 [==============================] - 51s 2s/step - loss: 1.1485 - accuracy: 0.5427 - val_loss: 1.3454 - val_accuracy: 0.4667\n","Epoch 12/20\n","30/30 [==============================] - 52s 2s/step - loss: 1.1085 - accuracy: 0.5646 - val_loss: 1.2483 - val_accuracy: 0.5333\n","Epoch 13/20\n","30/30 [==============================] - 52s 2s/step - loss: 1.1292 - accuracy: 0.5510 - val_loss: 1.3916 - val_accuracy: 0.5333\n","Epoch 14/20\n","30/30 [==============================] - 52s 2s/step - loss: 1.1697 - accuracy: 0.5323 - val_loss: 1.3606 - val_accuracy: 0.4917\n","Epoch 15/20\n","30/30 [==============================] - 50s 2s/step - loss: 1.0886 - accuracy: 0.5531 - val_loss: 1.0001 - val_accuracy: 0.6208\n","Epoch 16/20\n","30/30 [==============================] - 44s 1s/step - loss: 1.0612 - accuracy: 0.5844 - val_loss: 1.1476 - val_accuracy: 0.5417\n","Epoch 17/20\n","30/30 [==============================] - 48s 2s/step - loss: 0.9551 - accuracy: 0.6240 - val_loss: 1.2277 - val_accuracy: 0.5417\n","Epoch 18/20\n","30/30 [==============================] - 42s 1s/step - loss: 1.0845 - accuracy: 0.5594 - val_loss: 1.0727 - val_accuracy: 0.5792\n","Epoch 19/20\n","30/30 [==============================] - 50s 2s/step - loss: 0.9751 - accuracy: 0.6167 - val_loss: 1.0360 - val_accuracy: 0.6167\n","Epoch 20/20\n","30/30 [==============================] - 40s 1s/step - loss: 0.9503 - accuracy: 0.6271 - val_loss: 1.3967 - val_accuracy: 0.5208\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x20d4bd580a0>"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","\n","# 이미지 크기 설정\n","img_size = (224, 224)\n","max_images_per_category = 200\n","\n","# 데이터 로드 및 전처리 함수\n","def load_and_preprocess_data(image_paths, label, brightness=True):\n","    data = []\n","    labels = []\n","\n","    # 처음부터 200장만 선택\n","    selected_images = image_paths[:max_images_per_category]\n","\n","    for img_path in selected_images:\n","        img = cv2.imread(img_path)\n","\n","        # 이미지가 비어 있는지 확인\n","        if img is None:\n","            print(f\"이미지 로딩 중 오류 발생: {img_path}\")\n","            continue\n","\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, img_size)\n","\n","        # 이미지 명도와 채도 높이기\n","        if brightness:\n","            img = cv2.convertScaleAbs(img, alpha=1.2, beta=30)\n","\n","        img_array = img.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n","\n","        data.append(img_array)\n","        labels.append(label)\n","\n","    return np.array(data), np.array(labels)\n","\n","# 카테고리 리스트\n","categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n","num_classes = len(categories)\n","\n","# 데이터 로드 및 전처리 함수를 이용하여 데이터 읽기\n","all_data = []\n","all_labels = []\n","\n","for i, category in enumerate(categories):\n","    category_path = os.path.join(\"crawled_img\", category)\n","    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n","\n","    data, labels = load_and_preprocess_data(category_images, label=i)\n","    all_data.extend(data)\n","    all_labels.extend(labels)\n","\n","# 데이터 합치기 및 레이블 변환\n","X = np.array(all_data)\n","y = to_categorical(np.array(all_labels), num_classes=num_classes)\n","\n","# 학습 및 테스트 데이터로 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# CNN 모델 생성\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 데이터 증강을 적용하여 모델 훈련\n","datagen = ImageDataGenerator(\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","datagen.fit(X_train)\n","model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=20, validation_data=(X_test, y_test))\n"]},{"cell_type":"markdown","id":"73efc16a-e204-4875-a016-351aea6538fc","metadata":{"id":"73efc16a-e204-4875-a016-351aea6538fc"},"source":["### CNN 사용한 샘플모델링(데이터 증강, 알고리즘 sgd) - 이미지 200장"]},{"cell_type":"code","execution_count":null,"id":"6f6ee80b-bbd3-40c2-81a3-14a6d2f9d354","metadata":{"id":"6f6ee80b-bbd3-40c2-81a3-14a6d2f9d354","outputId":"d8e0dd1e-4f7b-45d1-ad2b-68480d9efbb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","30/30 [==============================] - 30s 956ms/step - loss: 1.7923 - accuracy: 0.1833 - val_loss: 1.7694 - val_accuracy: 0.1708\n","Epoch 2/20\n","30/30 [==============================] - 30s 987ms/step - loss: 1.7682 - accuracy: 0.2469 - val_loss: 1.7356 - val_accuracy: 0.1875\n","Epoch 3/20\n","30/30 [==============================] - 31s 1s/step - loss: 1.7237 - accuracy: 0.2469 - val_loss: 1.6622 - val_accuracy: 0.3792\n","Epoch 4/20\n","30/30 [==============================] - 31s 1s/step - loss: 1.6860 - accuracy: 0.2937 - val_loss: 1.5926 - val_accuracy: 0.3500\n","Epoch 5/20\n","30/30 [==============================] - 42s 1s/step - loss: 1.6818 - accuracy: 0.2854 - val_loss: 1.6450 - val_accuracy: 0.2292\n","Epoch 6/20\n","30/30 [==============================] - 53s 2s/step - loss: 1.6946 - accuracy: 0.2865 - val_loss: 1.6835 - val_accuracy: 0.2375\n","Epoch 7/20\n","30/30 [==============================] - 60s 2s/step - loss: 1.6482 - accuracy: 0.3167 - val_loss: 1.5887 - val_accuracy: 0.3542\n","Epoch 8/20\n","30/30 [==============================] - 59s 2s/step - loss: 1.5977 - accuracy: 0.3365 - val_loss: 1.5173 - val_accuracy: 0.4333\n","Epoch 9/20\n","30/30 [==============================] - 54s 2s/step - loss: 1.5373 - accuracy: 0.3500 - val_loss: 1.4608 - val_accuracy: 0.4000\n","Epoch 10/20\n","30/30 [==============================] - 51s 2s/step - loss: 1.5787 - accuracy: 0.3187 - val_loss: 1.5374 - val_accuracy: 0.3917\n","Epoch 11/20\n","30/30 [==============================] - 53s 2s/step - loss: 1.5679 - accuracy: 0.3458 - val_loss: 1.5789 - val_accuracy: 0.3125\n","Epoch 12/20\n","30/30 [==============================] - 43s 1s/step - loss: 1.5514 - accuracy: 0.3656 - val_loss: 1.4661 - val_accuracy: 0.4833\n","Epoch 13/20\n","30/30 [==============================] - 52s 2s/step - loss: 1.4844 - accuracy: 0.3729 - val_loss: 1.3995 - val_accuracy: 0.4708\n","Epoch 14/20\n","30/30 [==============================] - 43s 1s/step - loss: 1.5159 - accuracy: 0.3885 - val_loss: 1.6135 - val_accuracy: 0.3250\n","Epoch 15/20\n","30/30 [==============================] - 58s 2s/step - loss: 1.6545 - accuracy: 0.3302 - val_loss: 1.5822 - val_accuracy: 0.4458\n","Epoch 16/20\n","30/30 [==============================] - 50s 2s/step - loss: 1.5801 - accuracy: 0.3729 - val_loss: 1.4144 - val_accuracy: 0.4750\n","Epoch 17/20\n","30/30 [==============================] - 59s 2s/step - loss: 1.5376 - accuracy: 0.3771 - val_loss: 1.4278 - val_accuracy: 0.4542\n","Epoch 18/20\n","30/30 [==============================] - 50s 2s/step - loss: 1.4885 - accuracy: 0.4052 - val_loss: 1.4490 - val_accuracy: 0.4458\n","Epoch 19/20\n","30/30 [==============================] - 48s 2s/step - loss: 1.4462 - accuracy: 0.4260 - val_loss: 1.7001 - val_accuracy: 0.3083\n","Epoch 20/20\n","30/30 [==============================] - 46s 2s/step - loss: 1.4848 - accuracy: 0.3865 - val_loss: 1.4002 - val_accuracy: 0.4875\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x16c54ee8f40>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","\n","# 이미지 크기 설정\n","img_size = (224, 224)\n","max_images_per_category = 200\n","\n","# 데이터 로드 및 전처리 함수\n","def load_and_preprocess_data(image_paths, label, brightness=True):\n","    data = []\n","    labels = []\n","\n","    # 처음부터 200장만 선택\n","    selected_images = image_paths[:max_images_per_category]\n","\n","    for img_path in selected_images:\n","        img = cv2.imread(img_path)\n","\n","        # 이미지가 비어 있는지 확인\n","        if img is None:\n","            print(f\"이미지 로딩 중 오류 발생: {img_path}\")\n","            continue\n","\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, img_size)\n","\n","        # 이미지 명도와 채도 높이기\n","        if brightness:\n","            img = cv2.convertScaleAbs(img, alpha=1.2, beta=30)\n","\n","        img_array = img.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n","\n","        data.append(img_array)\n","        labels.append(label)\n","\n","    return np.array(data), np.array(labels)\n","\n","# 카테고리 리스트\n","categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n","num_classes = len(categories)\n","\n","# 데이터 로드 및 전처리 함수를 이용하여 데이터 읽기\n","all_data = []\n","all_labels = []\n","\n","for i, category in enumerate(categories):\n","    category_path = os.path.join(\"crawled_img\", category)\n","    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n","\n","    data, labels = load_and_preprocess_data(category_images, label=i)\n","    all_data.extend(data)\n","    all_labels.extend(labels)\n","\n","# 데이터 합치기 및 레이블 변환\n","X = np.array(all_data)\n","y = to_categorical(np.array(all_labels), num_classes=num_classes)\n","\n","# 학습 및 테스트 데이터로 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# CNN 모델 생성\n","model_sgd = Sequential()\n","model_sgd.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","model_sgd.add(MaxPooling2D((2, 2)))\n","model_sgd.add(Conv2D(64, (3, 3), activation='relu'))\n","model_sgd.add(MaxPooling2D((2, 2)))\n","model_sgd.add(Conv2D(128, (3, 3), activation='relu'))\n","model_sgd.add(MaxPooling2D((2, 2)))\n","model_sgd.add(Flatten())\n","model_sgd.add(Dense(128, activation='relu'))\n","model_sgd.add(Dense(num_classes, activation='softmax'))\n","\n","# 모델 컴파일 - 'sgd' 옵티마이저\n","model_sgd.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 데이터 증강을 적용하여 모델 훈련 - 'sgd' 모델\n","datagen_sgd = ImageDataGenerator(\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","datagen_sgd.fit(X_train)\n","model_sgd.fit(datagen_sgd.flow(X_train, y_train, batch_size=32), epochs=20, validation_data=(X_test, y_test))"]},{"cell_type":"markdown","id":"5eab6cc6-30a4-4329-8d88-67649373d409","metadata":{"id":"5eab6cc6-30a4-4329-8d88-67649373d409"},"source":["### CNN 사용한 샘플모델링(데이터 증강, 알고리즘 rmsprop) - 이미지 200장"]},{"cell_type":"code","execution_count":null,"id":"85dc6e3c-a975-41a1-80a8-5951ef410910","metadata":{"id":"85dc6e3c-a975-41a1-80a8-5951ef410910","outputId":"7b6ecb9b-a10c-4561-b3ad-6dfd14acb657"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","30/30 [==============================] - 43s 1s/step - loss: 3.6052 - accuracy: 0.1771 - val_loss: 1.7331 - val_accuracy: 0.2125\n","Epoch 2/20\n","30/30 [==============================] - 60s 2s/step - loss: 1.8132 - accuracy: 0.2458 - val_loss: 1.6640 - val_accuracy: 0.2333\n","Epoch 3/20\n","30/30 [==============================] - 65s 2s/step - loss: 1.6907 - accuracy: 0.2917 - val_loss: 1.4815 - val_accuracy: 0.4042\n","Epoch 4/20\n","30/30 [==============================] - 45s 1s/step - loss: 1.5725 - accuracy: 0.3365 - val_loss: 1.5665 - val_accuracy: 0.3292\n","Epoch 5/20\n","30/30 [==============================] - 56s 2s/step - loss: 1.6032 - accuracy: 0.3562 - val_loss: 1.3667 - val_accuracy: 0.4875\n","Epoch 6/20\n","30/30 [==============================] - 45s 1s/step - loss: 1.5710 - accuracy: 0.3948 - val_loss: 1.3209 - val_accuracy: 0.4750\n","Epoch 7/20\n","30/30 [==============================] - 53s 2s/step - loss: 1.4105 - accuracy: 0.4229 - val_loss: 1.2877 - val_accuracy: 0.4500\n","Epoch 8/20\n","30/30 [==============================] - 48s 2s/step - loss: 1.4381 - accuracy: 0.4187 - val_loss: 1.6407 - val_accuracy: 0.3292\n","Epoch 9/20\n","30/30 [==============================] - 50s 2s/step - loss: 1.4209 - accuracy: 0.4344 - val_loss: 1.2424 - val_accuracy: 0.4792\n","Epoch 10/20\n","30/30 [==============================] - 42s 1s/step - loss: 1.3318 - accuracy: 0.4823 - val_loss: 1.2587 - val_accuracy: 0.5250\n","Epoch 11/20\n","30/30 [==============================] - 51s 2s/step - loss: 1.3363 - accuracy: 0.4635 - val_loss: 1.2363 - val_accuracy: 0.4958\n","Epoch 12/20\n","30/30 [==============================] - 41s 1s/step - loss: 1.2520 - accuracy: 0.5052 - val_loss: 1.5495 - val_accuracy: 0.4250\n","Epoch 13/20\n","30/30 [==============================] - 51s 2s/step - loss: 1.3401 - accuracy: 0.4896 - val_loss: 1.8005 - val_accuracy: 0.3875\n","Epoch 14/20\n","30/30 [==============================] - 43s 1s/step - loss: 1.2449 - accuracy: 0.5156 - val_loss: 1.0365 - val_accuracy: 0.6083\n","Epoch 15/20\n","30/30 [==============================] - 47s 2s/step - loss: 1.2297 - accuracy: 0.5385 - val_loss: 1.0554 - val_accuracy: 0.6042\n","Epoch 16/20\n","30/30 [==============================] - 50s 2s/step - loss: 1.2554 - accuracy: 0.5323 - val_loss: 1.2756 - val_accuracy: 0.5000\n","Epoch 17/20\n","30/30 [==============================] - 38s 1s/step - loss: 1.2053 - accuracy: 0.5208 - val_loss: 0.9611 - val_accuracy: 0.6333\n","Epoch 18/20\n","30/30 [==============================] - 49s 2s/step - loss: 1.1801 - accuracy: 0.5479 - val_loss: 1.2023 - val_accuracy: 0.5500\n","Epoch 19/20\n","30/30 [==============================] - 48s 2s/step - loss: 1.1071 - accuracy: 0.5813 - val_loss: 3.9270 - val_accuracy: 0.2125\n","Epoch 20/20\n","30/30 [==============================] - 49s 2s/step - loss: 1.1880 - accuracy: 0.5635 - val_loss: 1.0517 - val_accuracy: 0.5625\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x16c5529f790>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","\n","# 이미지 크기 설정\n","img_size = (224, 224)\n","max_images_per_category = 200\n","\n","# 데이터 로드 및 전처리 함수\n","def load_and_preprocess_data(image_paths, label, brightness=True):\n","    data = []\n","    labels = []\n","\n","    # 처음부터 200장만 선택\n","    selected_images = image_paths[:max_images_per_category]\n","\n","    for img_path in selected_images:\n","        img = cv2.imread(img_path)\n","\n","        # 이미지가 비어 있는지 확인\n","        if img is None:\n","            print(f\"이미지 로딩 중 오류 발생: {img_path}\")\n","            continue\n","\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, img_size)\n","\n","        # 이미지 명도와 채도 높이기\n","        if brightness:\n","            img = cv2.convertScaleAbs(img, alpha=1.2, beta=30)\n","\n","        img_array = img.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n","\n","        data.append(img_array)\n","        labels.append(label)\n","\n","    return np.array(data), np.array(labels)\n","\n","# 카테고리 리스트\n","categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n","num_classes = len(categories)\n","\n","# 데이터 로드 및 전처리 함수를 이용하여 데이터 읽기\n","all_data = []\n","all_labels = []\n","\n","for i, category in enumerate(categories):\n","    category_path = os.path.join(\"crawled_img\", category)\n","    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n","\n","    data, labels = load_and_preprocess_data(category_images, label=i)\n","    all_data.extend(data)\n","    all_labels.extend(labels)\n","\n","# 데이터 합치기 및 레이블 변환\n","X = np.array(all_data)\n","y = to_categorical(np.array(all_labels), num_classes=num_classes)\n","\n","# 학습 및 테스트 데이터로 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# CNN 모델 생성 - 'rmsprop' 옵티마이저\n","model_rmsprop = Sequential()\n","model_rmsprop.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","model_rmsprop.add(MaxPooling2D((2, 2)))\n","model_rmsprop.add(Conv2D(64, (3, 3), activation='relu'))\n","model_rmsprop.add(MaxPooling2D((2, 2)))\n","model_rmsprop.add(Conv2D(128, (3, 3), activation='relu'))\n","model_rmsprop.add(MaxPooling2D((2, 2)))\n","model_rmsprop.add(Flatten())\n","model_rmsprop.add(Dense(128, activation='relu'))\n","model_rmsprop.add(Dense(num_classes, activation='softmax'))\n","\n","# 모델 컴파일 - 'rmsprop' 옵티마이저\n","model_rmsprop.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 데이터 증강을 적용하여 모델 훈련 - 'rmsprop' 모델\n","datagen_rmsprop = ImageDataGenerator(\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","datagen_rmsprop.fit(X_train)\n","model_rmsprop.fit(datagen_rmsprop.flow(X_train, y_train, batch_size=32), epochs=20, validation_data=(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"id":"d2ab42b7-2b60-4ba2-8ef0-aad337201425","metadata":{"id":"d2ab42b7-2b60-4ba2-8ef0-aad337201425"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"529013bc-ae7f-42f6-bf8a-0fce6f920515","metadata":{"id":"529013bc-ae7f-42f6-bf8a-0fce6f920515"},"source":["## 데이터 증강 이미지 200 에포크 40"]},{"cell_type":"code","execution_count":null,"id":"68b6ab40-4ca3-4cae-8496-b3e64d7db1d6","metadata":{"id":"68b6ab40-4ca3-4cae-8496-b3e64d7db1d6","outputId":"ae621dcc-6b6f-4245-da3a-9297fc83d468"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","30/30 [==============================] - 79s 3s/step - loss: 2.5464 - accuracy: 0.1708 - val_loss: 1.6597 - val_accuracy: 0.2917\n","Epoch 2/40\n","30/30 [==============================] - 60s 2s/step - loss: 1.6117 - accuracy: 0.3073 - val_loss: 1.8939 - val_accuracy: 0.2625\n","Epoch 3/40\n","30/30 [==============================] - 60s 2s/step - loss: 1.5272 - accuracy: 0.3531 - val_loss: 1.4012 - val_accuracy: 0.3750\n","Epoch 4/40\n","30/30 [==============================] - 54s 2s/step - loss: 1.4305 - accuracy: 0.3906 - val_loss: 1.9176 - val_accuracy: 0.3292\n","Epoch 5/40\n","30/30 [==============================] - 55s 2s/step - loss: 1.3523 - accuracy: 0.4500 - val_loss: 1.8667 - val_accuracy: 0.3375\n","Epoch 6/40\n","30/30 [==============================] - 58s 2s/step - loss: 1.3364 - accuracy: 0.4750 - val_loss: 1.2319 - val_accuracy: 0.5250\n","Epoch 7/40\n","30/30 [==============================] - 52s 2s/step - loss: 1.2652 - accuracy: 0.5010 - val_loss: 1.2202 - val_accuracy: 0.5167\n","Epoch 8/40\n","30/30 [==============================] - 52s 2s/step - loss: 1.1430 - accuracy: 0.5312 - val_loss: 1.0477 - val_accuracy: 0.5917\n","Epoch 9/40\n","30/30 [==============================] - 52s 2s/step - loss: 1.1324 - accuracy: 0.5521 - val_loss: 1.2831 - val_accuracy: 0.5042\n","Epoch 10/40\n","30/30 [==============================] - 53s 2s/step - loss: 1.1549 - accuracy: 0.5510 - val_loss: 1.1097 - val_accuracy: 0.5542\n","Epoch 11/40\n","30/30 [==============================] - 54s 2s/step - loss: 1.0498 - accuracy: 0.5896 - val_loss: 1.4109 - val_accuracy: 0.5125\n","Epoch 12/40\n","30/30 [==============================] - 54s 2s/step - loss: 1.0720 - accuracy: 0.5969 - val_loss: 2.1139 - val_accuracy: 0.3417\n","Epoch 13/40\n","30/30 [==============================] - 55s 2s/step - loss: 1.1394 - accuracy: 0.5604 - val_loss: 1.1001 - val_accuracy: 0.5375\n","Epoch 14/40\n","30/30 [==============================] - 62s 2s/step - loss: 1.0638 - accuracy: 0.5740 - val_loss: 1.7259 - val_accuracy: 0.3792\n","Epoch 15/40\n","30/30 [==============================] - 56s 2s/step - loss: 0.9496 - accuracy: 0.6344 - val_loss: 1.1886 - val_accuracy: 0.5667\n","Epoch 16/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.9756 - accuracy: 0.6156 - val_loss: 1.3652 - val_accuracy: 0.5083\n","Epoch 17/40\n","30/30 [==============================] - 53s 2s/step - loss: 1.0402 - accuracy: 0.5979 - val_loss: 1.2557 - val_accuracy: 0.5292\n","Epoch 18/40\n","30/30 [==============================] - 53s 2s/step - loss: 0.9768 - accuracy: 0.6323 - val_loss: 1.6897 - val_accuracy: 0.4625\n","Epoch 19/40\n","30/30 [==============================] - 53s 2s/step - loss: 1.0194 - accuracy: 0.5917 - val_loss: 1.2382 - val_accuracy: 0.5333\n","Epoch 20/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.9617 - accuracy: 0.6365 - val_loss: 1.1699 - val_accuracy: 0.5542\n","Epoch 21/40\n","30/30 [==============================] - 52s 2s/step - loss: 0.9201 - accuracy: 0.6396 - val_loss: 1.1819 - val_accuracy: 0.5625\n","Epoch 22/40\n","30/30 [==============================] - 52s 2s/step - loss: 0.9311 - accuracy: 0.6552 - val_loss: 1.3268 - val_accuracy: 0.5333\n","Epoch 23/40\n","30/30 [==============================] - 53s 2s/step - loss: 0.9627 - accuracy: 0.6250 - val_loss: 1.5608 - val_accuracy: 0.4875\n","Epoch 24/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.9261 - accuracy: 0.6375 - val_loss: 1.1856 - val_accuracy: 0.5625\n","Epoch 25/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.8181 - accuracy: 0.6833 - val_loss: 0.9812 - val_accuracy: 0.6458\n","Epoch 26/40\n","30/30 [==============================] - 52s 2s/step - loss: 0.9290 - accuracy: 0.6562 - val_loss: 0.9191 - val_accuracy: 0.6625\n","Epoch 27/40\n","30/30 [==============================] - 50s 2s/step - loss: 0.8956 - accuracy: 0.6427 - val_loss: 1.0579 - val_accuracy: 0.6250\n","Epoch 28/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.9078 - accuracy: 0.6635 - val_loss: 1.1080 - val_accuracy: 0.5875\n","Epoch 29/40\n","30/30 [==============================] - 52s 2s/step - loss: 0.8179 - accuracy: 0.7052 - val_loss: 1.3701 - val_accuracy: 0.5333\n","Epoch 30/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.8655 - accuracy: 0.6573 - val_loss: 1.2163 - val_accuracy: 0.5625\n","Epoch 31/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.7845 - accuracy: 0.7094 - val_loss: 1.1324 - val_accuracy: 0.6167\n","Epoch 32/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.7472 - accuracy: 0.7104 - val_loss: 1.3762 - val_accuracy: 0.5792\n","Epoch 33/40\n","30/30 [==============================] - 53s 2s/step - loss: 0.7924 - accuracy: 0.7073 - val_loss: 0.9584 - val_accuracy: 0.6542\n","Epoch 34/40\n","30/30 [==============================] - 50s 2s/step - loss: 0.8465 - accuracy: 0.6708 - val_loss: 1.7090 - val_accuracy: 0.4417\n","Epoch 35/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.7695 - accuracy: 0.6948 - val_loss: 1.2826 - val_accuracy: 0.5583\n","Epoch 36/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.7574 - accuracy: 0.6844 - val_loss: 1.2879 - val_accuracy: 0.5833\n","Epoch 37/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.7769 - accuracy: 0.7135 - val_loss: 1.3224 - val_accuracy: 0.5833\n","Epoch 38/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.7530 - accuracy: 0.7198 - val_loss: 1.4624 - val_accuracy: 0.5208\n","Epoch 39/40\n","30/30 [==============================] - 50s 2s/step - loss: 0.6688 - accuracy: 0.7531 - val_loss: 1.3114 - val_accuracy: 0.5708\n","Epoch 40/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.6975 - accuracy: 0.7229 - val_loss: 1.1844 - val_accuracy: 0.5917\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x2129e305d80>"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","\n","# 이미지 크기 설정\n","img_size = (224, 224)\n","max_images_per_category = 200\n","\n","# 데이터 로드 및 전처리 함수\n","def load_and_preprocess_data(image_paths, label, brightness=True):\n","    data = []\n","    labels = []\n","\n","    # 처음부터 200장만 선택\n","    selected_images = image_paths[:max_images_per_category]\n","\n","    for img_path in selected_images:\n","        img = cv2.imread(img_path)\n","\n","        # 이미지가 비어 있는지 확인\n","        if img is None:\n","            print(f\"이미지 로딩 중 오류 발생: {img_path}\")\n","            continue\n","\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, img_size)\n","\n","        # 이미지 명도와 채도 높이기\n","        if brightness:\n","            img = cv2.convertScaleAbs(img, alpha=1.2, beta=30)\n","\n","        img_array = img.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n","\n","        data.append(img_array)\n","        labels.append(label)\n","\n","    return np.array(data), np.array(labels)\n","\n","# 카테고리 리스트\n","categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n","num_classes = len(categories)\n","\n","# 데이터 로드 및 전처리 함수를 이용하여 데이터 읽기\n","all_data = []\n","all_labels = []\n","\n","for i, category in enumerate(categories):\n","    category_path = os.path.join(\"crawled_img\", category)\n","    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n","\n","    data, labels = load_and_preprocess_data(category_images, label=i)\n","    all_data.extend(data)\n","    all_labels.extend(labels)\n","\n","# 데이터 합치기 및 레이블 변환\n","X = np.array(all_data)\n","y = to_categorical(np.array(all_labels), num_classes=num_classes)\n","\n","# 학습 및 테스트 데이터로 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# CNN 모델 생성\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 데이터 증강을 적용하여 모델 훈련\n","datagen = ImageDataGenerator(\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","datagen.fit(X_train)\n","model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=40, validation_data=(X_test, y_test))\n"]},{"cell_type":"code","execution_count":null,"id":"585301ba-5f1c-443f-96dd-bb9682ca7115","metadata":{"id":"585301ba-5f1c-443f-96dd-bb9682ca7115","outputId":"a0e3a953-fccd-4311-f6e8-68d5f0d559fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["8/8 [==============================] - 2s 175ms/step - loss: 1.4169 - accuracy: 0.5708\n","테스트 손실: 1.4168516397476196\n","테스트 정확도: 0.5708333253860474\n"]}],"source":["# 모델 평가\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"테스트 손실: {test_loss}\")\n","print(f\"테스트 정확도: {test_accuracy}\")"]},{"cell_type":"code","execution_count":null,"id":"65870f90-d748-4497-b5e7-6069f48d42f7","metadata":{"id":"65870f90-d748-4497-b5e7-6069f48d42f7"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"e9267fef-562f-43bb-88a0-9ba4b3c57c17","metadata":{"id":"e9267fef-562f-43bb-88a0-9ba4b3c57c17"},"source":["### 데이터 증강 이미지 200 에포크 40 얼리스타핑"]},{"cell_type":"code","execution_count":null,"id":"44d023f2-4374-43f2-b893-852a872fe0d1","metadata":{"id":"44d023f2-4374-43f2-b893-852a872fe0d1","outputId":"4b4b1a80-f884-49da-e8d9-891d8d59a6c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","30/30 [==============================] - 29s 935ms/step - loss: 1.8752 - accuracy: 0.2604 - val_loss: 1.6074 - val_accuracy: 0.3417\n","Epoch 2/40\n","30/30 [==============================] - 30s 992ms/step - loss: 1.5372 - accuracy: 0.3375 - val_loss: 1.6017 - val_accuracy: 0.3625\n","Epoch 3/40\n","30/30 [==============================] - 31s 1s/step - loss: 1.4657 - accuracy: 0.4031 - val_loss: 1.8217 - val_accuracy: 0.2667\n","Epoch 4/40\n","30/30 [==============================] - 31s 1s/step - loss: 1.3872 - accuracy: 0.4417 - val_loss: 1.3241 - val_accuracy: 0.5000\n","Epoch 5/40\n","30/30 [==============================] - 34s 1s/step - loss: 1.3616 - accuracy: 0.4479 - val_loss: 1.3528 - val_accuracy: 0.4542\n","Epoch 6/40\n","30/30 [==============================] - 53s 2s/step - loss: 1.3030 - accuracy: 0.4760 - val_loss: 1.3088 - val_accuracy: 0.4750\n","Epoch 7/40\n","30/30 [==============================] - 55s 2s/step - loss: 1.2098 - accuracy: 0.5354 - val_loss: 1.0663 - val_accuracy: 0.5708\n","Epoch 8/40\n","30/30 [==============================] - 54s 2s/step - loss: 1.1594 - accuracy: 0.5448 - val_loss: 1.3274 - val_accuracy: 0.4833\n","Epoch 9/40\n","30/30 [==============================] - 54s 2s/step - loss: 1.1808 - accuracy: 0.5323 - val_loss: 1.5345 - val_accuracy: 0.4500\n","Epoch 10/40\n","30/30 [==============================] - 55s 2s/step - loss: 1.2020 - accuracy: 0.5260 - val_loss: 1.2113 - val_accuracy: 0.5375\n","Epoch 11/40\n","30/30 [==============================] - 65s 2s/step - loss: 1.1027 - accuracy: 0.5677 - val_loss: 1.1920 - val_accuracy: 0.5542\n","Epoch 12/40\n","30/30 [==============================] - 51s 2s/step - loss: 1.0855 - accuracy: 0.5719 - val_loss: 1.4065 - val_accuracy: 0.5417\n","Epoch 13/40\n","30/30 [==============================] - 55s 2s/step - loss: 1.0301 - accuracy: 0.5927 - val_loss: 1.2741 - val_accuracy: 0.5583\n","Epoch 14/40\n","30/30 [==============================] - 56s 2s/step - loss: 1.0556 - accuracy: 0.5927 - val_loss: 1.0436 - val_accuracy: 0.5958\n","Epoch 15/40\n","30/30 [==============================] - 53s 2s/step - loss: 1.0093 - accuracy: 0.6073 - val_loss: 1.6565 - val_accuracy: 0.5042\n","Epoch 16/40\n","30/30 [==============================] - 81s 3s/step - loss: 0.9788 - accuracy: 0.6125 - val_loss: 1.1623 - val_accuracy: 0.5625\n","Epoch 17/40\n","30/30 [==============================] - 53s 2s/step - loss: 0.9782 - accuracy: 0.6260 - val_loss: 0.8578 - val_accuracy: 0.6750\n","Epoch 18/40\n","30/30 [==============================] - 53s 2s/step - loss: 0.9806 - accuracy: 0.6198 - val_loss: 1.0552 - val_accuracy: 0.6208\n","Epoch 19/40\n","30/30 [==============================] - 57s 2s/step - loss: 0.9199 - accuracy: 0.6438 - val_loss: 0.9752 - val_accuracy: 0.6417\n","Epoch 20/40\n","30/30 [==============================] - 71s 2s/step - loss: 0.8858 - accuracy: 0.6635 - val_loss: 1.2631 - val_accuracy: 0.5917\n","Epoch 21/40\n","30/30 [==============================] - 58s 2s/step - loss: 0.9098 - accuracy: 0.6583 - val_loss: 1.6298 - val_accuracy: 0.5375\n","Epoch 22/40\n","30/30 [==============================] - 52s 2s/step - loss: 0.8850 - accuracy: 0.6469 - val_loss: 1.3538 - val_accuracy: 0.5667\n","Epoch 23/40\n","30/30 [==============================] - 52s 2s/step - loss: 0.8643 - accuracy: 0.6792 - val_loss: 1.1115 - val_accuracy: 0.6208\n","Epoch 24/40\n","30/30 [==============================] - 54s 2s/step - loss: 0.8201 - accuracy: 0.6708 - val_loss: 1.1728 - val_accuracy: 0.6000\n","Epoch 25/40\n","30/30 [==============================] - 55s 2s/step - loss: 0.7815 - accuracy: 0.7031 - val_loss: 1.4302 - val_accuracy: 0.5875\n","Epoch 26/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.8187 - accuracy: 0.6896 - val_loss: 2.1521 - val_accuracy: 0.4583\n","Epoch 27/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.8551 - accuracy: 0.6823 - val_loss: 1.7749 - val_accuracy: 0.5375\n","Epoch 28/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.8477 - accuracy: 0.6792 - val_loss: 1.2248 - val_accuracy: 0.5958\n","Epoch 29/40\n","30/30 [==============================] - 52s 2s/step - loss: 0.8243 - accuracy: 0.6927 - val_loss: 1.4127 - val_accuracy: 0.5833\n","Epoch 30/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.7585 - accuracy: 0.7135 - val_loss: 0.9533 - val_accuracy: 0.6667\n","Epoch 31/40\n","30/30 [==============================] - 51s 2s/step - loss: 0.7957 - accuracy: 0.7042 - val_loss: 1.2474 - val_accuracy: 0.5625\n","Epoch 32/40\n","30/30 [==============================] - 52s 2s/step - loss: 0.7291 - accuracy: 0.7115 - val_loss: 1.0153 - val_accuracy: 0.6375\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# 이미지 크기 설정\n","img_size = (224, 224)\n","max_images_per_category = 200\n","\n","# 데이터 로드 및 전처리 함수\n","def load_and_preprocess_data(image_paths, label, brightness=True):\n","    data = []\n","    labels = []\n","\n","    # 처음부터 200장만 선택\n","    selected_images = image_paths[:max_images_per_category]\n","\n","    for img_path in selected_images:\n","        img = cv2.imread(img_path)\n","\n","        # 이미지가 비어 있는지 확인\n","        if img is None:\n","            print(f\"이미지 로딩 중 오류 발생: {img_path}\")\n","            continue\n","\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, img_size)\n","\n","        # 이미지 명도와 채도 높이기\n","        if brightness:\n","            img = cv2.convertScaleAbs(img, alpha=1.2, beta=30)\n","\n","        img_array = img.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n","\n","        data.append(img_array)\n","        labels.append(label)\n","\n","    return np.array(data), np.array(labels)\n","\n","# 카테고리 리스트\n","categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n","num_classes = len(categories)\n","\n","# 데이터 로드 및 전처리 함수를 이용하여 데이터 읽기\n","all_data = []\n","all_labels = []\n","\n","for i, category in enumerate(categories):\n","    category_path = os.path.join(\"crawled_img\", category)\n","    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n","\n","    data, labels = load_and_preprocess_data(category_images, label=i)\n","    all_data.extend(data)\n","    all_labels.extend(labels)\n","\n","# 데이터 합치기 및 레이블 변환\n","X = np.array(all_data)\n","y = to_categorical(np.array(all_labels), num_classes=num_classes)\n","\n","# 학습 및 테스트 데이터로 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# CNN 모델 생성\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D((2, 2)))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# EarlyStopping 콜백 정의\n","early_stopping = EarlyStopping(monitor='val_accuracy',  # 모니터링할 지표\n","                               patience=15,             # 지정된 epochs 동안 개선이 없을 경우 학습 중지\n","                               restore_best_weights=True)  # 가장 좋은 가중치로 복원\n","\n","# 데이터 증강을 적용하여 모델 훈련\n","datagen = ImageDataGenerator(\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","datagen.fit(X_train)\n","history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n","                    epochs=40,\n","                    validation_data=(X_test, y_test),\n","                    callbacks=[early_stopping])\n"]},{"cell_type":"code","execution_count":null,"id":"19efb43a-9618-406f-b1d7-3096dbc569cb","metadata":{"id":"19efb43a-9618-406f-b1d7-3096dbc569cb","outputId":"b028edf6-86f0-4241-efc9-1efb05958317"},"outputs":[{"name":"stdout","output_type":"stream","text":["8/8 [==============================] - 1s 155ms/step - loss: 0.8578 - accuracy: 0.6750\n","Test Accuracy: 0.6750\n","Test Loss: 0.8578\n"]}],"source":["# 테스트 데이터에서 모델 평가\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","print(f\"Test Loss: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"b5dbf15e-afb3-4289-bb09-11ff17107d41","metadata":{"id":"b5dbf15e-afb3-4289-bb09-11ff17107d41"},"outputs":[],"source":["from tensorflow.keras.models import save_model, load_model\n","\n","# 전체 모델 저장\n","model.save(\"keyword_image.h5\")\n","\n","# 전체 모델 불러오기\n","loaded_model = load_model(\"keyword_image.h5\")"]},{"cell_type":"code","execution_count":null,"id":"657231de-41f2-41c2-a196-3c5d1fb045f4","metadata":{"id":"657231de-41f2-41c2-a196-3c5d1fb045f4"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ca10c58c-055c-4e79-be3f-f9b25ae1f5ad","metadata":{"id":"ca10c58c-055c-4e79-be3f-f9b25ae1f5ad"},"source":["### 데이터 증강, 이미지 200, 에포크 60"]},{"cell_type":"code","execution_count":null,"id":"1a95c4ea-9276-4f0e-b65b-2a7ef1fb84c4","metadata":{"id":"1a95c4ea-9276-4f0e-b65b-2a7ef1fb84c4","outputId":"beb15bb7-c38f-493c-9990-258d85adbf72"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/60\n","30/30 [==============================] - 29s 906ms/step - loss: 2.0089 - accuracy: 0.2115 - val_loss: 1.6545 - val_accuracy: 0.3042\n","Epoch 2/60\n","30/30 [==============================] - 30s 982ms/step - loss: 1.5587 - accuracy: 0.3396 - val_loss: 1.9513 - val_accuracy: 0.2667\n","Epoch 3/60\n","30/30 [==============================] - 30s 1s/step - loss: 1.4897 - accuracy: 0.3656 - val_loss: 1.6041 - val_accuracy: 0.3750\n","Epoch 4/60\n","30/30 [==============================] - 31s 1s/step - loss: 1.4408 - accuracy: 0.4208 - val_loss: 1.8316 - val_accuracy: 0.3333\n","Epoch 5/60\n","30/30 [==============================] - 34s 1s/step - loss: 1.3689 - accuracy: 0.4583 - val_loss: 1.7818 - val_accuracy: 0.3333\n","Epoch 6/60\n","30/30 [==============================] - 50s 2s/step - loss: 1.3081 - accuracy: 0.4865 - val_loss: 1.3914 - val_accuracy: 0.4333\n","Epoch 7/60\n","30/30 [==============================] - 64s 2s/step - loss: 1.2542 - accuracy: 0.5031 - val_loss: 1.5296 - val_accuracy: 0.4125\n","Epoch 8/60\n","30/30 [==============================] - 52s 2s/step - loss: 1.2435 - accuracy: 0.5208 - val_loss: 1.1168 - val_accuracy: 0.5792\n","Epoch 9/60\n","30/30 [==============================] - 51s 2s/step - loss: 1.1470 - accuracy: 0.5344 - val_loss: 1.0257 - val_accuracy: 0.5583\n","Epoch 10/60\n","30/30 [==============================] - 53s 2s/step - loss: 1.1213 - accuracy: 0.5688 - val_loss: 1.4276 - val_accuracy: 0.4750\n","Epoch 11/60\n","30/30 [==============================] - 53s 2s/step - loss: 1.1338 - accuracy: 0.5583 - val_loss: 1.1129 - val_accuracy: 0.5500\n","Epoch 12/60\n","30/30 [==============================] - 72s 2s/step - loss: 1.1114 - accuracy: 0.5646 - val_loss: 1.0825 - val_accuracy: 0.5792\n","Epoch 13/60\n","30/30 [==============================] - 52s 2s/step - loss: 1.1007 - accuracy: 0.5615 - val_loss: 1.1996 - val_accuracy: 0.5292\n","Epoch 14/60\n","30/30 [==============================] - 52s 2s/step - loss: 1.0968 - accuracy: 0.5833 - val_loss: 1.0299 - val_accuracy: 0.5917\n","Epoch 15/60\n","30/30 [==============================] - 53s 2s/step - loss: 1.0294 - accuracy: 0.5979 - val_loss: 1.5811 - val_accuracy: 0.4875\n","Epoch 16/60\n","30/30 [==============================] - 55s 2s/step - loss: 1.0972 - accuracy: 0.5490 - val_loss: 1.5143 - val_accuracy: 0.4458\n","Epoch 17/60\n","30/30 [==============================] - 52s 2s/step - loss: 1.0242 - accuracy: 0.6021 - val_loss: 1.1537 - val_accuracy: 0.5500\n","Epoch 18/60\n","30/30 [==============================] - 51s 2s/step - loss: 1.0265 - accuracy: 0.5958 - val_loss: 1.4117 - val_accuracy: 0.5042\n","Epoch 19/60\n","30/30 [==============================] - 52s 2s/step - loss: 0.9881 - accuracy: 0.6042 - val_loss: 1.5114 - val_accuracy: 0.4958\n","Epoch 20/60\n","30/30 [==============================] - 50s 2s/step - loss: 0.9912 - accuracy: 0.6010 - val_loss: 1.7361 - val_accuracy: 0.4375\n","Epoch 21/60\n","30/30 [==============================] - 52s 2s/step - loss: 0.9843 - accuracy: 0.6083 - val_loss: 1.3463 - val_accuracy: 0.5375\n","Epoch 22/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.9434 - accuracy: 0.6302 - val_loss: 1.3687 - val_accuracy: 0.5292\n","Epoch 23/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.9290 - accuracy: 0.6438 - val_loss: 1.3083 - val_accuracy: 0.5625\n","Epoch 24/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.9025 - accuracy: 0.6417 - val_loss: 1.0816 - val_accuracy: 0.5958\n","Epoch 25/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.9418 - accuracy: 0.6323 - val_loss: 1.0259 - val_accuracy: 0.5917\n","Epoch 26/60\n","30/30 [==============================] - 50s 2s/step - loss: 0.9056 - accuracy: 0.6521 - val_loss: 1.3625 - val_accuracy: 0.5458\n","Epoch 27/60\n","30/30 [==============================] - 50s 2s/step - loss: 0.8431 - accuracy: 0.6781 - val_loss: 1.1354 - val_accuracy: 0.5917\n","Epoch 28/60\n","30/30 [==============================] - 52s 2s/step - loss: 0.8599 - accuracy: 0.6750 - val_loss: 1.0925 - val_accuracy: 0.6000\n","Epoch 29/60\n","30/30 [==============================] - 57s 2s/step - loss: 0.8366 - accuracy: 0.6625 - val_loss: 0.9829 - val_accuracy: 0.6292\n","Epoch 30/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.8558 - accuracy: 0.6802 - val_loss: 1.3480 - val_accuracy: 0.5875\n","Epoch 31/60\n","30/30 [==============================] - 55s 2s/step - loss: 0.7825 - accuracy: 0.6917 - val_loss: 1.5463 - val_accuracy: 0.5667\n","Epoch 32/60\n","30/30 [==============================] - 53s 2s/step - loss: 0.8333 - accuracy: 0.6823 - val_loss: 1.2913 - val_accuracy: 0.5625\n","Epoch 33/60\n","30/30 [==============================] - 53s 2s/step - loss: 0.7956 - accuracy: 0.6979 - val_loss: 1.2173 - val_accuracy: 0.5833\n","Epoch 34/60\n","30/30 [==============================] - 54s 2s/step - loss: 0.8023 - accuracy: 0.6948 - val_loss: 1.1807 - val_accuracy: 0.6125\n","Epoch 35/60\n","30/30 [==============================] - 52s 2s/step - loss: 0.8158 - accuracy: 0.6896 - val_loss: 1.2279 - val_accuracy: 0.6083\n","Epoch 36/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.7517 - accuracy: 0.7031 - val_loss: 1.2939 - val_accuracy: 0.6083\n","Epoch 37/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.7063 - accuracy: 0.7094 - val_loss: 1.1137 - val_accuracy: 0.6333\n","Epoch 38/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.8325 - accuracy: 0.6938 - val_loss: 1.2428 - val_accuracy: 0.5875\n","Epoch 39/60\n","30/30 [==============================] - 54s 2s/step - loss: 0.8128 - accuracy: 0.6948 - val_loss: 1.2950 - val_accuracy: 0.5875\n","Epoch 40/60\n","30/30 [==============================] - 52s 2s/step - loss: 0.6896 - accuracy: 0.7458 - val_loss: 0.9461 - val_accuracy: 0.6792\n","Epoch 41/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.7405 - accuracy: 0.7271 - val_loss: 1.2971 - val_accuracy: 0.5875\n","Epoch 42/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.8416 - accuracy: 0.6771 - val_loss: 1.7062 - val_accuracy: 0.5208\n","Epoch 43/60\n","30/30 [==============================] - 49s 2s/step - loss: 0.7314 - accuracy: 0.7156 - val_loss: 1.3359 - val_accuracy: 0.6042\n","Epoch 44/60\n","30/30 [==============================] - 49s 2s/step - loss: 0.7139 - accuracy: 0.7219 - val_loss: 1.3080 - val_accuracy: 0.6083\n","Epoch 45/60\n","30/30 [==============================] - 49s 2s/step - loss: 0.7711 - accuracy: 0.6969 - val_loss: 1.0230 - val_accuracy: 0.6250\n","Epoch 46/60\n","30/30 [==============================] - 50s 2s/step - loss: 0.8187 - accuracy: 0.7052 - val_loss: 0.9596 - val_accuracy: 0.6250\n","Epoch 47/60\n","30/30 [==============================] - 48s 2s/step - loss: 0.7214 - accuracy: 0.7250 - val_loss: 1.3074 - val_accuracy: 0.5958\n","Epoch 48/60\n","30/30 [==============================] - 50s 2s/step - loss: 0.7290 - accuracy: 0.7312 - val_loss: 1.4901 - val_accuracy: 0.5708\n","Epoch 49/60\n","30/30 [==============================] - 49s 2s/step - loss: 0.6619 - accuracy: 0.7510 - val_loss: 0.9494 - val_accuracy: 0.6458\n","Epoch 50/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.6806 - accuracy: 0.7375 - val_loss: 1.2441 - val_accuracy: 0.6250\n","Epoch 51/60\n","30/30 [==============================] - 49s 2s/step - loss: 0.6608 - accuracy: 0.7458 - val_loss: 1.3773 - val_accuracy: 0.6167\n","Epoch 52/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.7272 - accuracy: 0.7188 - val_loss: 1.0135 - val_accuracy: 0.6375\n","Epoch 53/60\n","30/30 [==============================] - 48s 2s/step - loss: 0.6527 - accuracy: 0.7458 - val_loss: 1.1569 - val_accuracy: 0.6375\n","Epoch 54/60\n","30/30 [==============================] - 51s 2s/step - loss: 0.6601 - accuracy: 0.7542 - val_loss: 1.4935 - val_accuracy: 0.5792\n","Epoch 55/60\n","30/30 [==============================] - 65s 2s/step - loss: 0.6564 - accuracy: 0.7594 - val_loss: 1.0986 - val_accuracy: 0.6208\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# 이미지 크기 설정\n","img_size = (224, 224)\n","max_images_per_category = 200\n","\n","# 데이터 로드 및 전처리 함수\n","def load_and_preprocess_data(image_paths, label, brightness=True):\n","    data = []\n","    labels = []\n","\n","    # 처음부터 200장만 선택\n","    selected_images = image_paths[:max_images_per_category]\n","\n","    for img_path in selected_images:\n","        img = cv2.imread(img_path)\n","\n","        # 이미지가 비어 있는지 확인\n","        if img is None:\n","            print(f\"이미지 로딩 중 오류 발생: {img_path}\")\n","            continue\n","\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, img_size)\n","\n","        # 이미지 명도와 채도 높이기\n","        if brightness:\n","            img = cv2.convertScaleAbs(img, alpha=1.2, beta=30)\n","\n","        img_array = img.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n","\n","        data.append(img_array)\n","        labels.append(label)\n","\n","    return np.array(data), np.array(labels)\n","\n","# 카테고리 리스트\n","categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n","num_classes = len(categories)\n","\n","# 데이터 로드 및 전처리 함수를 이용하여 데이터 읽기\n","all_data = []\n","all_labels = []\n","\n","for i, category in enumerate(categories):\n","    category_path = os.path.join(\"crawled_img\", category)\n","    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n","\n","    data, labels = load_and_preprocess_data(category_images, label=i)\n","    all_data.extend(data)\n","    all_labels.extend(labels)\n","\n","# 데이터 합치기 및 레이블 변환\n","X = np.array(all_data)\n","y = to_categorical(np.array(all_labels), num_classes=num_classes)\n","\n","# 학습 및 테스트 데이터로 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# CNN 모델 생성\n","model1 = Sequential()\n","model1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n","model1.add(MaxPooling2D((2, 2)))\n","model1.add(Conv2D(64, (3, 3), activation='relu'))\n","model1.add(MaxPooling2D((2, 2)))\n","model1.add(Conv2D(128, (3, 3), activation='relu'))\n","model1.add(MaxPooling2D((2, 2)))\n","model1.add(Flatten())\n","model1.add(Dense(128, activation='relu'))\n","model1.add(Dense(num_classes, activation='softmax'))\n","\n","# 모델 컴파일\n","model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# EarlyStopping 콜백 정의\n","early_stopping = EarlyStopping(monitor='val_accuracy',  # 모니터링할 지표\n","                               patience=15,             # 지정된 epochs 동안 개선이 없을 경우 학습 중지\n","                               restore_best_weights=True)  # 가장 좋은 가중치로 복원\n","\n","# 데이터 증강을 적용하여 모델 훈련\n","datagen = ImageDataGenerator(\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","datagen.fit(X_train)\n","history = model1.fit(datagen.flow(X_train, y_train, batch_size=32),\n","                    epochs=60,\n","                    validation_data=(X_test, y_test),\n","                    callbacks=[early_stopping])\n"]},{"cell_type":"markdown","id":"0604f8fc-2b8b-4c74-90af-5b1f0665e6d4","metadata":{"id":"0604f8fc-2b8b-4c74-90af-5b1f0665e6d4"},"source":["- epoch 수를 늘리수록 train_loss랑 train_accuracy는 수치가 좋아짐\n","- val_loss는 평균 0.5~0.6 수치를 보임"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}