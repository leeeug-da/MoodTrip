{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1500e7f3-bd3b-4ea3-b073-fe4e08b4f1ff",
   "metadata": {},
   "source": [
    "### 6개키워드 중 하나만 나오게 하는 샘플모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb648cb2-9c39-41e4-903d-54e79767a6c0",
   "metadata": {},
   "source": [
    "## ./crawled_img/(분위기키워드 폴더명)\n",
    " - joyful(활발한)\n",
    " - adventure(모험적인)\n",
    " - cultural(문화적인)\n",
    " - tradition(전통적인)\n",
    " - nature(자연)\n",
    " - art(예술적인)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac69d7-7abd-462e-8a22-8ca7b60ce628",
   "metadata": {},
   "source": [
    "### CNN 사용한 샘플모델링(데이터 증강X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b16194f-fe91-47f6-bbc3-b15c5274d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 53s 4s/step - loss: 2.7246 - accuracy: 0.2167 - val_loss: 1.6795 - val_accuracy: 0.3417\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 48s 3s/step - loss: 1.5757 - accuracy: 0.3333 - val_loss: 1.5144 - val_accuracy: 0.3250\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 45s 3s/step - loss: 1.4213 - accuracy: 0.4313 - val_loss: 1.5768 - val_accuracy: 0.3750\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 46s 3s/step - loss: 1.3605 - accuracy: 0.4979 - val_loss: 1.4591 - val_accuracy: 0.4917\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 43s 3s/step - loss: 1.1661 - accuracy: 0.5646 - val_loss: 1.4295 - val_accuracy: 0.4250\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 43s 3s/step - loss: 1.0173 - accuracy: 0.6271 - val_loss: 1.3254 - val_accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 41s 3s/step - loss: 0.8664 - accuracy: 0.6833 - val_loss: 1.4641 - val_accuracy: 0.4917\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 45s 3s/step - loss: 0.7887 - accuracy: 0.7458 - val_loss: 1.2290 - val_accuracy: 0.5167\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 43s 3s/step - loss: 0.5456 - accuracy: 0.7979 - val_loss: 1.4177 - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 44s 3s/step - loss: 0.3416 - accuracy: 0.8813 - val_loss: 1.4996 - val_accuracy: 0.5167\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 43s 3s/step - loss: 0.2534 - accuracy: 0.9417 - val_loss: 1.9474 - val_accuracy: 0.4917\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 43s 3s/step - loss: 0.2648 - accuracy: 0.9187 - val_loss: 1.8673 - val_accuracy: 0.4750\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 50s 3s/step - loss: 0.1106 - accuracy: 0.9688 - val_loss: 1.9279 - val_accuracy: 0.5417\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 75s 5s/step - loss: 0.0412 - accuracy: 0.9937 - val_loss: 2.3298 - val_accuracy: 0.5250\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 52s 3s/step - loss: 0.0607 - accuracy: 0.9875 - val_loss: 2.2752 - val_accuracy: 0.4917\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 48s 3s/step - loss: 0.0726 - accuracy: 0.9854 - val_loss: 1.9005 - val_accuracy: 0.5500\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 42s 3s/step - loss: 0.0728 - accuracy: 0.9854 - val_loss: 2.4926 - val_accuracy: 0.4667\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 43s 3s/step - loss: 0.0348 - accuracy: 0.9958 - val_loss: 2.3969 - val_accuracy: 0.5167\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 42s 3s/step - loss: 0.0132 - accuracy: 0.9979 - val_loss: 2.4442 - val_accuracy: 0.5000\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 42s 3s/step - loss: 0.0116 - accuracy: 0.9979 - val_loss: 2.5842 - val_accuracy: 0.5333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d7580b340>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# 이미지 크기 설정\n",
    "img_size = (224, 224) # 이미지 크키가 크면 메모리를 많이 차지해서 설정 필요\n",
    "\n",
    "# 데이터 로드 및 전처리 함수\n",
    "def load_and_preprocess_data(image_paths, label, brightness=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 랜덤하게 100장만 선택\n",
    "    selected_images = random.sample(image_paths, 100) #300개 이미지\n",
    "\n",
    "    for img_path in selected_images:\n",
    "        img = load_img(img_path, target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "\n",
    "        # 이미지 명도와 채도 높이기\n",
    "        if brightness:\n",
    "            img_array = cv2.convertScaleAbs(img_array, alpha=1.2, beta=30)\n",
    "\n",
    "        img_array = img_array.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n",
    "\n",
    "        data.append(img_array)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "# 카테고리 리스트\n",
    "categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# 이미지 데이터 로드 및 전처리\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_path = os.path.join(\"crawled_img\", category) # crawled_img/안에 있는 카테고리 리스트\n",
    "    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n",
    "    \n",
    "    data, labels = load_and_preprocess_data(category_images, label=i)\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# 데이터 합치기 및 레이블 변환\n",
    "X = np.array(all_data)\n",
    "y = to_categorical(np.array(all_labels), num_classes=num_classes)\n",
    "\n",
    "# 학습 및 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94b1b8-0c18-4320-a932-16d3f682a68c",
   "metadata": {},
   "source": [
    "### CNN 사용한 샘플모델링(데이터 증강) - 이미지 100장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2a0a2f4-8aab-42c3-afd8-3a8656d679c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 84s 5s/step - loss: 2.4894 - accuracy: 0.1583 - val_loss: 1.7986 - val_accuracy: 0.1417\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 71s 4s/step - loss: 1.7684 - accuracy: 0.1813 - val_loss: 1.6924 - val_accuracy: 0.2750\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 59s 4s/step - loss: 1.6336 - accuracy: 0.2854 - val_loss: 1.5882 - val_accuracy: 0.3917\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 56s 4s/step - loss: 1.5683 - accuracy: 0.3688 - val_loss: 1.8033 - val_accuracy: 0.3417\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 57s 4s/step - loss: 1.5395 - accuracy: 0.3417 - val_loss: 1.4774 - val_accuracy: 0.4000\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 54s 4s/step - loss: 1.5360 - accuracy: 0.3938 - val_loss: 1.4210 - val_accuracy: 0.3833\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 51s 3s/step - loss: 1.5132 - accuracy: 0.3646 - val_loss: 1.5818 - val_accuracy: 0.3833\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 59s 4s/step - loss: 1.4514 - accuracy: 0.4042 - val_loss: 1.6657 - val_accuracy: 0.3833\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 69s 4s/step - loss: 1.3984 - accuracy: 0.4354 - val_loss: 1.3278 - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 56s 4s/step - loss: 1.3085 - accuracy: 0.4667 - val_loss: 1.3463 - val_accuracy: 0.5083\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 57s 4s/step - loss: 1.2800 - accuracy: 0.5042 - val_loss: 1.2240 - val_accuracy: 0.5250\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 54s 3s/step - loss: 1.3585 - accuracy: 0.4667 - val_loss: 1.5476 - val_accuracy: 0.4000\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 50s 3s/step - loss: 1.2949 - accuracy: 0.5104 - val_loss: 1.3615 - val_accuracy: 0.4750\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 50s 3s/step - loss: 1.2303 - accuracy: 0.5250 - val_loss: 1.5750 - val_accuracy: 0.4417\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 48s 3s/step - loss: 1.3155 - accuracy: 0.4354 - val_loss: 1.3772 - val_accuracy: 0.4917\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 49s 3s/step - loss: 1.2351 - accuracy: 0.5375 - val_loss: 1.3362 - val_accuracy: 0.5333\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 52s 3s/step - loss: 1.1752 - accuracy: 0.5479 - val_loss: 1.3003 - val_accuracy: 0.5583\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 51s 3s/step - loss: 1.1674 - accuracy: 0.5458 - val_loss: 1.3992 - val_accuracy: 0.5250\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 59s 4s/step - loss: 1.2100 - accuracy: 0.5375 - val_loss: 1.0928 - val_accuracy: 0.6250\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 53s 3s/step - loss: 1.0628 - accuracy: 0.6042 - val_loss: 1.2225 - val_accuracy: 0.5583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d756909d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# 이미지 크기 설정\n",
    "img_size = (224, 224)\n",
    "\n",
    "# 데이터 로드 및 전처리 함수\n",
    "def load_and_preprocess_data(image_paths, label, brightness=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 랜덤하게 100장만 선택\n",
    "    selected_images = random.sample(image_paths, 100)\n",
    "\n",
    "    for img_path in selected_images:\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # 이미지가 비어 있는지 확인\n",
    "        if img is None:\n",
    "            print(f\"이미지 로딩 중 오류 발생: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, img_size)\n",
    "\n",
    "        # 이미지 명도와 채도 높이기\n",
    "        if brightness:\n",
    "            img = cv2.convertScaleAbs(img, alpha=1.2, beta=30)\n",
    "\n",
    "        img_array = img.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n",
    "\n",
    "        data.append(img_array)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# 카테고리 리스트\n",
    "categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# 데이터 로드 및 전처리 함수를 이용하여 데이터 읽기\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_path = os.path.join(\"crawled_img\", category)\n",
    "    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n",
    "    \n",
    "    data, labels = load_and_preprocess_data(category_images, label=i)\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# 데이터 합치기 및 레이블 변환\n",
    "X = np.array(all_data)\n",
    "y = to_categorical(np.array(all_labels), num_classes=num_classes)\n",
    "\n",
    "# 학습 및 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 데이터 증강을 적용하여 모델 훈련\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=20, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73efc16a-e204-4875-a016-351aea6538fc",
   "metadata": {},
   "source": [
    "### CNN 사용한 샘플모델링(데이터 증강) - 이미지 300장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f6ee80b-bbd3-40c2-81a3-14a6d2f9d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "45/45 [==============================] - 172s 4s/step - loss: 2.0617 - accuracy: 0.1813 - val_loss: 1.7111 - val_accuracy: 0.2583\n",
      "Epoch 2/20\n",
      "45/45 [==============================] - 163s 4s/step - loss: 1.7504 - accuracy: 0.2410 - val_loss: 1.7167 - val_accuracy: 0.2833\n",
      "Epoch 3/20\n",
      "45/45 [==============================] - 157s 3s/step - loss: 1.6036 - accuracy: 0.3326 - val_loss: 1.4596 - val_accuracy: 0.4167\n",
      "Epoch 4/20\n",
      "45/45 [==============================] - 204s 5s/step - loss: 1.4978 - accuracy: 0.3840 - val_loss: 1.3543 - val_accuracy: 0.4556\n",
      "Epoch 5/20\n",
      "45/45 [==============================] - 153s 3s/step - loss: 1.3808 - accuracy: 0.4569 - val_loss: 1.5203 - val_accuracy: 0.3889\n",
      "Epoch 6/20\n",
      "45/45 [==============================] - 140s 3s/step - loss: 1.3289 - accuracy: 0.4646 - val_loss: 1.2940 - val_accuracy: 0.4667\n",
      "Epoch 7/20\n",
      "45/45 [==============================] - 144s 3s/step - loss: 1.2468 - accuracy: 0.5000 - val_loss: 1.6545 - val_accuracy: 0.3556\n",
      "Epoch 8/20\n",
      "45/45 [==============================] - 141s 3s/step - loss: 1.2455 - accuracy: 0.5042 - val_loss: 1.3658 - val_accuracy: 0.4528\n",
      "Epoch 9/20\n",
      "45/45 [==============================] - 139s 3s/step - loss: 1.2320 - accuracy: 0.5208 - val_loss: 1.5006 - val_accuracy: 0.3750\n",
      "Epoch 10/20\n",
      "45/45 [==============================] - 152s 3s/step - loss: 1.2244 - accuracy: 0.5063 - val_loss: 1.2255 - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "45/45 [==============================] - 154s 3s/step - loss: 1.1355 - accuracy: 0.5681 - val_loss: 1.1279 - val_accuracy: 0.5639\n",
      "Epoch 12/20\n",
      "45/45 [==============================] - 151s 3s/step - loss: 1.1264 - accuracy: 0.5646 - val_loss: 1.4450 - val_accuracy: 0.4583\n",
      "Epoch 13/20\n",
      "45/45 [==============================] - 154s 3s/step - loss: 1.1257 - accuracy: 0.5653 - val_loss: 1.2826 - val_accuracy: 0.5056\n",
      "Epoch 14/20\n",
      "45/45 [==============================] - 152s 3s/step - loss: 1.0804 - accuracy: 0.5715 - val_loss: 1.2468 - val_accuracy: 0.5194\n",
      "Epoch 15/20\n",
      "45/45 [==============================] - 139s 3s/step - loss: 1.0462 - accuracy: 0.5875 - val_loss: 1.4043 - val_accuracy: 0.5083\n",
      "Epoch 16/20\n",
      "45/45 [==============================] - 165s 4s/step - loss: 1.0481 - accuracy: 0.5840 - val_loss: 1.1118 - val_accuracy: 0.5611\n",
      "Epoch 17/20\n",
      "45/45 [==============================] - 160s 3s/step - loss: 1.0279 - accuracy: 0.5847 - val_loss: 1.5173 - val_accuracy: 0.4389\n",
      "Epoch 18/20\n",
      "45/45 [==============================] - 166s 4s/step - loss: 1.0391 - accuracy: 0.6042 - val_loss: 1.3257 - val_accuracy: 0.5139\n",
      "Epoch 19/20\n",
      "45/45 [==============================] - 149s 3s/step - loss: 0.9446 - accuracy: 0.6278 - val_loss: 1.4432 - val_accuracy: 0.5139\n",
      "Epoch 20/20\n",
      "45/45 [==============================] - 120s 3s/step - loss: 1.0158 - accuracy: 0.6174 - val_loss: 1.2754 - val_accuracy: 0.5083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20d4aa2dff0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# 이미지 크기 설정\n",
    "img_size = (224, 224)\n",
    "\n",
    "# 데이터 로드 및 전처리 함수\n",
    "def load_and_preprocess_data(image_paths, label, brightness=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 랜덤하게 300장만 선택\n",
    "    selected_images = random.sample(image_paths, 300)\n",
    "\n",
    "    for img_path in selected_images:\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # 이미지가 비어 있는지 확인\n",
    "        if img is None:\n",
    "            print(f\"이미지 로딩 중 오류 발생: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, img_size)\n",
    "\n",
    "        # 이미지 명도와 채도 높이기\n",
    "        if brightness:\n",
    "            img = cv2.convertScaleAbs(img, alpha=1.2, beta=30)\n",
    "\n",
    "        img_array = img.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n",
    "\n",
    "        data.append(img_array)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# 카테고리 리스트\n",
    "categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# 데이터 로드 및 전처리 함수를 이용하여 데이터 읽기\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_path = os.path.join(\"crawled_img\", category)\n",
    "    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n",
    "    \n",
    "    data, labels = load_and_preprocess_data(category_images, label=i)\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# 데이터 합치기 및 레이블 변환\n",
    "X = np.array(all_data)\n",
    "y = to_categorical(np.array(all_labels), num_classes=num_classes)\n",
    "\n",
    "# 학습 및 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 데이터 증강을 적용하여 모델 훈련\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=20, validation_data=(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
