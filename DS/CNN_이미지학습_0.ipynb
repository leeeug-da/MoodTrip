{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1500e7f3-bd3b-4ea3-b073-fe4e08b4f1ff",
   "metadata": {},
   "source": [
    "### 6개키워드 중 하나만 나오게 하는 샘플모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb648cb2-9c39-41e4-903d-54e79767a6c0",
   "metadata": {},
   "source": [
    "## ./crawled_img/(분위기키워드 폴더명)\n",
    " - joyful(활발한)\n",
    " - adventure(모험적인)\n",
    " - cultural(문화적인)\n",
    " - tradition(전통적인)\n",
    " - nature(자연)\n",
    " - art(예술적인)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac69d7-7abd-462e-8a22-8ca7b60ce628",
   "metadata": {},
   "source": [
    "### CNN 사용한 샘플모델링(데이터 증강X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b16194f-fe91-47f6-bbc3-b15c5274d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "150/150 [==============================] - 396s 3s/step - loss: 1.5086 - accuracy: 0.4067 - val_loss: 1.2578 - val_accuracy: 0.5050\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 361s 2s/step - loss: 1.1150 - accuracy: 0.5756 - val_loss: 1.0760 - val_accuracy: 0.5775\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 312s 2s/step - loss: 0.8595 - accuracy: 0.6825 - val_loss: 0.8565 - val_accuracy: 0.6808\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 313s 2s/step - loss: 0.6426 - accuracy: 0.7677 - val_loss: 0.9079 - val_accuracy: 0.6642\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 306s 2s/step - loss: 0.4483 - accuracy: 0.8508 - val_loss: 1.1248 - val_accuracy: 0.6517\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 327s 2s/step - loss: 0.2220 - accuracy: 0.9265 - val_loss: 1.1591 - val_accuracy: 0.6875\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 299s 2s/step - loss: 0.0966 - accuracy: 0.9710 - val_loss: 1.5454 - val_accuracy: 0.6658\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 294s 2s/step - loss: 0.0532 - accuracy: 0.9862 - val_loss: 1.6204 - val_accuracy: 0.6808\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 300s 2s/step - loss: 0.0573 - accuracy: 0.9821 - val_loss: 1.8134 - val_accuracy: 0.6842\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 372s 2s/step - loss: 0.0337 - accuracy: 0.9919 - val_loss: 1.7842 - val_accuracy: 0.6883\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 317s 2s/step - loss: 0.0390 - accuracy: 0.9887 - val_loss: 1.8214 - val_accuracy: 0.6400\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 264s 2s/step - loss: 0.0381 - accuracy: 0.9879 - val_loss: 2.0174 - val_accuracy: 0.6900\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 259s 2s/step - loss: 0.0400 - accuracy: 0.9860 - val_loss: 2.4997 - val_accuracy: 0.6450\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 265s 2s/step - loss: 0.0594 - accuracy: 0.9821 - val_loss: 1.9841 - val_accuracy: 0.6667\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 262s 2s/step - loss: 0.0271 - accuracy: 0.9921 - val_loss: 2.0051 - val_accuracy: 0.6742\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 260s 2s/step - loss: 0.0243 - accuracy: 0.9925 - val_loss: 2.3393 - val_accuracy: 0.6575\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 257s 2s/step - loss: 0.0306 - accuracy: 0.9912 - val_loss: 2.2153 - val_accuracy: 0.6750\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 263s 2s/step - loss: 0.0147 - accuracy: 0.9958 - val_loss: 2.2944 - val_accuracy: 0.6642\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 253s 2s/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 2.3744 - val_accuracy: 0.6783\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 257s 2s/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 2.2817 - val_accuracy: 0.6850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c5e9b1ed40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# 이미지 크기 설정\n",
    "img_size = (224, 224) # 이미지 크키가 크면 메모리를 많이 차지해서 설정 필요\n",
    "\n",
    "# 데이터 로드 및 전처리 함수\n",
    "def load_and_preprocess_data(image_paths, label, brightness=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 랜덤하게 100장만 선택\n",
    "    selected_images = random.sample(image_paths, 1000) #1000개 이미지\n",
    "\n",
    "    for img_path in selected_images:\n",
    "        img = load_img(img_path, target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "\n",
    "        # 이미지 명도와 채도 높이기\n",
    "        if brightness:\n",
    "            img_array = cv2.convertScaleAbs(img_array, alpha=1.2, beta=30)\n",
    "\n",
    "        img_array = img_array.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n",
    "\n",
    "        data.append(img_array)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "# 카테고리 리스트\n",
    "categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# 이미지 데이터 로드 및 전처리\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_path = os.path.join(\"C:\\\\Users\\\\김현\\\\Final_Project\\\\crawled_img\", category) # crawled_img/안에 있는 카테고리 리스트\n",
    "    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n",
    "    \n",
    "    data, labels = load_and_preprocess_data(category_images, label=i)\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# 데이터 합치기 및 레이블 변환\n",
    "X = np.array(all_data)\n",
    "y = to_categorical(np.array(all_labels), num_classes=num_classes)\n",
    "\n",
    "# 학습 및 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a1339a3-e885-4975-818c-005e5d7a62e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 14s 381ms/step - loss: 2.2817 - accuracy: 0.6850\n",
      "테스트 손실: 2.2817\n",
      "테스트 정확도: 68.50%\n"
     ]
    }
   ],
   "source": [
    "# 테스트 세트에서 모델 평가\n",
    "evaluation_results = model.evaluate(X_test, y_test)\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(f'테스트 손실: {evaluation_results[0]:.4f}')\n",
    "print(f'테스트 정확도: {evaluation_results[1]*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94b1b8-0c18-4320-a932-16d3f682a68c",
   "metadata": {},
   "source": [
    "### 기존코드 + Dropout  -- 이미지 1000장\n",
    "모델의 과적합을 방지하고 성능을 개선하기 위한 방법 중 Dropout이라는 기법을 사용하기위한 코드\n",
    "\n",
    "Dropout은 과적합을 막기 위해 학습 과정 중 무작위로 일부 뉴런을 끄는 방법이다\r\n",
    "\r\n",
    "아래는 Dropout을 추가한 다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e608b685-c153-4042-936d-bb8c32f335e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "150/150 [==============================] - 289s 2s/step - loss: 1.7839 - accuracy: 0.3038 - val_loss: 1.4786 - val_accuracy: 0.3575\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 282s 2s/step - loss: 1.4079 - accuracy: 0.4338 - val_loss: 1.3247 - val_accuracy: 0.4625\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 277s 2s/step - loss: 1.2638 - accuracy: 0.5117 - val_loss: 1.1763 - val_accuracy: 0.5600\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 276s 2s/step - loss: 1.1902 - accuracy: 0.5448 - val_loss: 1.1813 - val_accuracy: 0.5558\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 276s 2s/step - loss: 1.1376 - accuracy: 0.5675 - val_loss: 1.1082 - val_accuracy: 0.5792\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 275s 2s/step - loss: 1.0895 - accuracy: 0.5831 - val_loss: 1.0739 - val_accuracy: 0.5858\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 273s 2s/step - loss: 1.0016 - accuracy: 0.6204 - val_loss: 1.0597 - val_accuracy: 0.5958\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 275s 2s/step - loss: 0.9418 - accuracy: 0.6473 - val_loss: 1.0183 - val_accuracy: 0.6117\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 278s 2s/step - loss: 0.8780 - accuracy: 0.6729 - val_loss: 0.9601 - val_accuracy: 0.6325\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 3356s 23s/step - loss: 0.7952 - accuracy: 0.6965 - val_loss: 0.9678 - val_accuracy: 0.6367\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 277s 2s/step - loss: 0.7185 - accuracy: 0.7262 - val_loss: 1.0057 - val_accuracy: 0.6358\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 274s 2s/step - loss: 0.6345 - accuracy: 0.7631 - val_loss: 0.9534 - val_accuracy: 0.6467\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 274s 2s/step - loss: 0.5648 - accuracy: 0.7852 - val_loss: 1.0079 - val_accuracy: 0.6425\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 274s 2s/step - loss: 0.5204 - accuracy: 0.8019 - val_loss: 1.0896 - val_accuracy: 0.6517\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 278s 2s/step - loss: 0.4718 - accuracy: 0.8250 - val_loss: 1.2405 - val_accuracy: 0.6208\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 273s 2s/step - loss: 0.4443 - accuracy: 0.8306 - val_loss: 1.1487 - val_accuracy: 0.6417\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 277s 2s/step - loss: 0.3854 - accuracy: 0.8544 - val_loss: 1.1711 - val_accuracy: 0.6567\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 273s 2s/step - loss: 0.3558 - accuracy: 0.8690 - val_loss: 1.1707 - val_accuracy: 0.6517\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 274s 2s/step - loss: 0.3256 - accuracy: 0.8821 - val_loss: 1.2915 - val_accuracy: 0.6517\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 274s 2s/step - loss: 0.3132 - accuracy: 0.8856 - val_loss: 1.3235 - val_accuracy: 0.6458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1be01503af0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# 이미지 크기 설정\n",
    "img_size = (224, 224) # 이미지 크키가 크면 메모리를 많이 차지해서 설정 필요\n",
    "\n",
    "# 데이터 로드 및 전처리 함수\n",
    "def load_and_preprocess_data(image_paths, label, brightness=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 랜덤하게 100장만 선택\n",
    "    selected_images = random.sample(image_paths, 1000) #1000개 이미지\n",
    "\n",
    "    for img_path in selected_images:\n",
    "        img = load_img(img_path, target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "\n",
    "        # 이미지 명도와 채도 높이기\n",
    "        if brightness:\n",
    "            img_array = cv2.convertScaleAbs(img_array, alpha=1.2, beta=30)\n",
    "\n",
    "        img_array = img_array.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n",
    "\n",
    "        data.append(img_array)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "# 카테고리 리스트\n",
    "categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# 이미지 데이터 로드 및 전처리\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_path = os.path.join(\"C:\\\\Users\\\\김현\\\\Final_Project\\\\crawled_img\", category) # crawled_img/안에 있는 카테고리 리스트\n",
    "    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n",
    "    \n",
    "    data, labels = load_and_preprocess_data(category_images, label=i)\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# 데이터 합치기 및 레이블 변환\n",
    "X = np.array(all_data)\n",
    "y = to_categorical(np.array(all_labels), num_classes=num_classes)\n",
    "\n",
    "# 학습 및 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout 추가\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e11d1fef-de43-459a-bc6a-9be42a233143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 14s 375ms/step - loss: 1.3235 - accuracy: 0.6458\n",
      "테스트 손실: 1.3235\n",
      "테스트 정확도: 64.58%\n"
     ]
    }
   ],
   "source": [
    "# 테스트 세트에서 모델 평가\n",
    "evaluation_results = model.evaluate(X_test, y_test)\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(f'테스트 손실: {evaluation_results[0]:.4f}')\n",
    "print(f'테스트 정확도: {evaluation_results[1]*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e712c9-bc8f-4217-a678-0400407c6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "# 전체 모델 저장\n",
    "model.save(\"C:/Users/김현/Final_Project/epoch20.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc554740-2235-40fa-9f3d-ebbf6609c405",
   "metadata": {},
   "source": [
    "### epoch 50으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f2cf34-5901-4282-b1c2-eca1686aa1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "150/150 [==============================] - 287s 2s/step - loss: 1.9314 - accuracy: 0.2365 - val_loss: 1.6191 - val_accuracy: 0.3217\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 282s 2s/step - loss: 1.5261 - accuracy: 0.3706 - val_loss: 1.4644 - val_accuracy: 0.4633\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 279s 2s/step - loss: 1.3347 - accuracy: 0.4944 - val_loss: 1.3219 - val_accuracy: 0.4942\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 292s 2s/step - loss: 1.2721 - accuracy: 0.5167 - val_loss: 1.2786 - val_accuracy: 0.5033\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 439s 3s/step - loss: 1.1682 - accuracy: 0.5500 - val_loss: 1.1555 - val_accuracy: 0.5775\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 568s 4s/step - loss: 1.0794 - accuracy: 0.5929 - val_loss: 1.2137 - val_accuracy: 0.5450\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 696s 5s/step - loss: 0.9710 - accuracy: 0.6402 - val_loss: 1.0660 - val_accuracy: 0.6017\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 714s 5s/step - loss: 0.8697 - accuracy: 0.6827 - val_loss: 0.9951 - val_accuracy: 0.6433\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 502s 3s/step - loss: 0.7300 - accuracy: 0.7333 - val_loss: 0.8925 - val_accuracy: 0.6883\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 502s 3s/step - loss: 0.6328 - accuracy: 0.7704 - val_loss: 0.9040 - val_accuracy: 0.6867\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 487s 3s/step - loss: 0.5453 - accuracy: 0.8023 - val_loss: 0.8830 - val_accuracy: 0.6925\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 523s 3s/step - loss: 0.4988 - accuracy: 0.8215 - val_loss: 0.9449 - val_accuracy: 0.6883\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 493s 3s/step - loss: 0.4491 - accuracy: 0.8335 - val_loss: 1.0263 - val_accuracy: 0.6908\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 446s 3s/step - loss: 0.3574 - accuracy: 0.8725 - val_loss: 1.0242 - val_accuracy: 0.7050\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 460s 3s/step - loss: 0.3194 - accuracy: 0.8852 - val_loss: 1.0606 - val_accuracy: 0.7017\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 464s 3s/step - loss: 0.2879 - accuracy: 0.8973 - val_loss: 1.2863 - val_accuracy: 0.6650\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 439s 3s/step - loss: 0.2498 - accuracy: 0.9110 - val_loss: 1.2567 - val_accuracy: 0.6950\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 480s 3s/step - loss: 0.2404 - accuracy: 0.9102 - val_loss: 1.1374 - val_accuracy: 0.6958\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 481s 3s/step - loss: 0.2212 - accuracy: 0.9246 - val_loss: 1.2283 - val_accuracy: 0.7025\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 458s 3s/step - loss: 0.2104 - accuracy: 0.9279 - val_loss: 1.4237 - val_accuracy: 0.6692\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 418s 3s/step - loss: 0.1977 - accuracy: 0.9312 - val_loss: 1.2824 - val_accuracy: 0.6833\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 428s 3s/step - loss: 0.1676 - accuracy: 0.9410 - val_loss: 1.3951 - val_accuracy: 0.6908\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 408s 3s/step - loss: 0.1894 - accuracy: 0.9287 - val_loss: 1.3039 - val_accuracy: 0.6892\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 515s 3s/step - loss: 0.1608 - accuracy: 0.9400 - val_loss: 1.5503 - val_accuracy: 0.6975\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 381s 3s/step - loss: 0.1681 - accuracy: 0.9400 - val_loss: 1.3827 - val_accuracy: 0.6933\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 402s 3s/step - loss: 0.1355 - accuracy: 0.9519 - val_loss: 1.4335 - val_accuracy: 0.6808\n",
      "Epoch 27/50\n",
      "150/150 [==============================] - 335s 2s/step - loss: 0.1468 - accuracy: 0.9448 - val_loss: 1.4208 - val_accuracy: 0.7042\n",
      "Epoch 28/50\n",
      "150/150 [==============================] - 296s 2s/step - loss: 0.1273 - accuracy: 0.9517 - val_loss: 1.4916 - val_accuracy: 0.7017\n",
      "Epoch 29/50\n",
      "150/150 [==============================] - 369s 2s/step - loss: 0.1362 - accuracy: 0.9525 - val_loss: 1.5063 - val_accuracy: 0.7000\n",
      "Epoch 30/50\n",
      "150/150 [==============================] - 438s 3s/step - loss: 0.1474 - accuracy: 0.9479 - val_loss: 1.7169 - val_accuracy: 0.6967\n",
      "Epoch 31/50\n",
      "150/150 [==============================] - 444s 3s/step - loss: 0.1367 - accuracy: 0.9519 - val_loss: 1.5693 - val_accuracy: 0.7167\n",
      "Epoch 32/50\n",
      "150/150 [==============================] - 454s 3s/step - loss: 0.1296 - accuracy: 0.9525 - val_loss: 1.5706 - val_accuracy: 0.7067\n",
      "Epoch 33/50\n",
      "150/150 [==============================] - 493s 3s/step - loss: 0.1195 - accuracy: 0.9577 - val_loss: 1.7086 - val_accuracy: 0.6850\n",
      "Epoch 34/50\n",
      "150/150 [==============================] - 461s 3s/step - loss: 0.1349 - accuracy: 0.9556 - val_loss: 1.5214 - val_accuracy: 0.6950\n",
      "Epoch 35/50\n",
      "150/150 [==============================] - 461s 3s/step - loss: 0.1232 - accuracy: 0.9585 - val_loss: 1.8344 - val_accuracy: 0.6908\n",
      "Epoch 36/50\n",
      "150/150 [==============================] - 439s 3s/step - loss: 0.1089 - accuracy: 0.9604 - val_loss: 1.4744 - val_accuracy: 0.7283\n",
      "Epoch 37/50\n",
      "150/150 [==============================] - 441s 3s/step - loss: 0.1103 - accuracy: 0.9600 - val_loss: 1.6250 - val_accuracy: 0.7108\n",
      "Epoch 38/50\n",
      "150/150 [==============================] - 401s 3s/step - loss: 0.1059 - accuracy: 0.9606 - val_loss: 1.7138 - val_accuracy: 0.6992\n",
      "Epoch 39/50\n",
      "150/150 [==============================] - 361s 2s/step - loss: 0.1161 - accuracy: 0.9588 - val_loss: 1.6155 - val_accuracy: 0.6933\n",
      "Epoch 40/50\n",
      "150/150 [==============================] - 416s 3s/step - loss: nan - accuracy: 0.6573 - val_loss: nan - val_accuracy: 0.1792\n",
      "Epoch 41/50\n",
      "150/150 [==============================] - 382s 3s/step - loss: nan - accuracy: 0.1635 - val_loss: nan - val_accuracy: 0.1792\n",
      "Epoch 42/50\n",
      "150/150 [==============================] - 376s 3s/step - loss: nan - accuracy: 0.1635 - val_loss: nan - val_accuracy: 0.1792\n",
      "Epoch 43/50\n",
      "150/150 [==============================] - 276s 2s/step - loss: nan - accuracy: 0.1635 - val_loss: nan - val_accuracy: 0.1792\n",
      "Epoch 44/50\n",
      "150/150 [==============================] - 266s 2s/step - loss: nan - accuracy: 0.1635 - val_loss: nan - val_accuracy: 0.1792\n",
      "Epoch 45/50\n",
      "150/150 [==============================] - 266s 2s/step - loss: nan - accuracy: 0.1635 - val_loss: nan - val_accuracy: 0.1792\n",
      "Epoch 46/50\n",
      "150/150 [==============================] - 266s 2s/step - loss: nan - accuracy: 0.1635 - val_loss: nan - val_accuracy: 0.1792\n",
      "Epoch 47/50\n",
      "150/150 [==============================] - 265s 2s/step - loss: nan - accuracy: 0.1635 - val_loss: nan - val_accuracy: 0.1792\n",
      "Epoch 48/50\n",
      "150/150 [==============================] - 269s 2s/step - loss: nan - accuracy: 0.1635 - val_loss: nan - val_accuracy: 0.1792\n",
      "Epoch 49/50\n",
      "150/150 [==============================] - 268s 2s/step - loss: nan - accuracy: 0.1635 - val_loss: nan - val_accuracy: 0.1792\n",
      "Epoch 50/50\n",
      "150/150 [==============================] - 2393s 16s/step - loss: nan - accuracy: 0.1635 - val_loss: nan - val_accuracy: 0.1792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1be1a080070>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# 이미지 크기 설정\n",
    "img_size = (224, 224) # 이미지 크키가 크면 메모리를 많이 차지해서 설정 필요\n",
    "\n",
    "# 데이터 로드 및 전처리 함수\n",
    "def load_and_preprocess_data(image_paths, label, brightness=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 랜덤하게 100장만 선택\n",
    "    selected_images = random.sample(image_paths, 1000) #1000개 이미지\n",
    "\n",
    "    for img_path in selected_images:\n",
    "        img = load_img(img_path, target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "\n",
    "        # 이미지 명도와 채도 높이기\n",
    "        if brightness:\n",
    "            img_array = cv2.convertScaleAbs(img_array, alpha=1.2, beta=30)\n",
    "\n",
    "        img_array = img_array.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n",
    "\n",
    "        data.append(img_array)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "# 카테고리 리스트\n",
    "categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# 이미지 데이터 로드 및 전처리\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_path = os.path.join(\"C:\\\\Users\\\\김현\\\\Final_Project\\\\crawled_img\", category) # crawled_img/안에 있는 카테고리 리스트\n",
    "    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n",
    "    \n",
    "    data, labels = load_and_preprocess_data(category_images, label=i)\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# 데이터 합치기 및 레이블 변환\n",
    "X = np.array(all_data)\n",
    "y = to_categorical(np.array(all_labels), num_classes=num_classes)\n",
    "\n",
    "# 학습 및 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout 추가\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0685b22a-e5d7-4c17-93cf-527e3934a372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 14s 372ms/step - loss: nan - accuracy: 0.1792\n",
      "테스트 손실: nan\n",
      "테스트 정확도: 17.92%\n"
     ]
    }
   ],
   "source": [
    "# 테스트 세트에서 모델 평가\n",
    "evaluation_results = model.evaluate(X_test, y_test)\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(f'테스트 손실: {evaluation_results[0]:.4f}')\n",
    "print(f'테스트 정확도: {evaluation_results[1]*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ced979d5-2e31-4f34-97e1-60994f8cba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "# 전체 모델 저장\n",
    "model.save(\"C:/Users/김현/Final_Project/epoch50.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6cefe-cdd5-475f-afc1-a6bf0a487ec8",
   "metadata": {},
   "source": [
    "## 기존코드 + Dropout + 데이터 증강  -- 1000장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "767c8bc0-9dc1-4982-b9c2-e3c5c8a0ff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "150/150 [==============================] - 400s 3s/step - loss: 1.9646 - accuracy: 0.1773 - val_loss: 1.7875 - val_accuracy: 0.1842\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 344s 2s/step - loss: 1.7612 - accuracy: 0.2240 - val_loss: 1.7619 - val_accuracy: 0.2192\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 403s 3s/step - loss: 1.7456 - accuracy: 0.2504 - val_loss: 1.7400 - val_accuracy: 0.2500\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 321s 2s/step - loss: 1.7343 - accuracy: 0.2438 - val_loss: 1.7519 - val_accuracy: 0.2283\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 315s 2s/step - loss: 1.7107 - accuracy: 0.2592 - val_loss: 1.6953 - val_accuracy: 0.2933\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 311s 2s/step - loss: 1.6943 - accuracy: 0.2606 - val_loss: 1.6080 - val_accuracy: 0.3358\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 307s 2s/step - loss: 1.6349 - accuracy: 0.3063 - val_loss: 1.5680 - val_accuracy: 0.3642\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 305s 2s/step - loss: 1.5822 - accuracy: 0.3465 - val_loss: 1.5072 - val_accuracy: 0.4375\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 315s 2s/step - loss: 1.5606 - accuracy: 0.3529 - val_loss: 1.5302 - val_accuracy: 0.4242\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 315s 2s/step - loss: 1.4508 - accuracy: 0.4090 - val_loss: 1.4218 - val_accuracy: 0.4550\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 310s 2s/step - loss: 1.3774 - accuracy: 0.4394 - val_loss: 1.2747 - val_accuracy: 0.5242\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 306s 2s/step - loss: 1.3511 - accuracy: 0.4569 - val_loss: 1.1882 - val_accuracy: 0.5433\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 312s 2s/step - loss: 1.2917 - accuracy: 0.4900 - val_loss: 1.1640 - val_accuracy: 0.5692\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 337s 2s/step - loss: 1.2390 - accuracy: 0.5038 - val_loss: 1.1560 - val_accuracy: 0.5575\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 384s 3s/step - loss: 1.2041 - accuracy: 0.5288 - val_loss: 1.1678 - val_accuracy: 0.5850\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 377s 3s/step - loss: 1.1993 - accuracy: 0.5383 - val_loss: 1.1198 - val_accuracy: 0.5908\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 326s 2s/step - loss: 1.1517 - accuracy: 0.5558 - val_loss: 1.0950 - val_accuracy: 0.6108\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 335s 2s/step - loss: 1.1377 - accuracy: 0.5685 - val_loss: 1.1563 - val_accuracy: 0.5633\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 320s 2s/step - loss: 1.1140 - accuracy: 0.5735 - val_loss: 1.1511 - val_accuracy: 0.5950\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 348s 2s/step - loss: 1.0881 - accuracy: 0.5815 - val_loss: 1.0229 - val_accuracy: 0.6233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c5e9ce9a20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# 이미지 크기 설정\n",
    "img_size = (224, 224)\n",
    "\n",
    "# 데이터 로드 및 전처리 함수\n",
    "def load_and_preprocess_data(image_paths, label, brightness=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 랜덤하게 100장만 선택\n",
    "    selected_images = random.sample(image_paths, 1000) #1000개 이미지\n",
    "\n",
    "    for img_path in selected_images:\n",
    "        img = load_img(img_path, target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "\n",
    "        # 이미지 명도와 채도 높이기\n",
    "        if brightness:\n",
    "            img_array = cv2.convertScaleAbs(img_array, alpha=1.2, beta=30)\n",
    "\n",
    "        img_array = img_array.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n",
    "\n",
    "        data.append(img_array)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# 카테고리 리스트\n",
    "categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# 이미지 데이터 로드 및 전처리\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_path = os.path.join(\"C:\\\\Users\\\\김현\\\\Final_Project\\\\crawled_img\", category) # crawled_img/안에 있는 카테고리 리스트\n",
    "    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n",
    "    \n",
    "    data, labels = load_and_preprocess_data(category_images, label=i)\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# 데이터 합치기 및 레이블 변환\n",
    "X = np.array(all_data)\n",
    "y = to_categorical(np.array(all_labels), num_classes=num_classes)\n",
    "\n",
    "# 학습 및 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 데이터 증강 설정\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,    # 이미지 회전 범위\n",
    "    width_shift_range=0.2,    # 이미지 수평 이동 범위\n",
    "    height_shift_range=0.2,   # 이미지 수직 이동 범위\n",
    "    zoom_range=0.2,   # 이미지 확대/축소 범위\n",
    "    horizontal_flip=True)  # 이미지 수평 뒤집기\n",
    "\n",
    "# 이미지 데이터 증강\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "          epochs=20,\n",
    "          validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d34fc63-4124-4bd2-8bfb-147244d1f8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 15s 403ms/step - loss: 1.0229 - accuracy: 0.6233\n",
      "테스트 손실: 1.0229\n",
      "테스트 정확도: 62.33%\n"
     ]
    }
   ],
   "source": [
    "# 테스트 세트에서 모델 평가\n",
    "evaluation_results = model.evaluate(X_test, y_test)\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(f'테스트 손실: {evaluation_results[0]:.4f}')\n",
    "print(f'테스트 정확도: {evaluation_results[1]*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7205c-e1cf-4e4e-b624-48a03805f553",
   "metadata": {},
   "source": [
    "### 기존코드(에포크50)에서 학습률을 낮추고, Batch Normalization 레이어를 추가하며, EarlyStopping을 적용한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "778f241b-9230-472a-962b-79b30a41e758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "150/150 [==============================] - 422s 3s/step - loss: 1.9860 - accuracy: 0.2465 - val_loss: 32.6843 - val_accuracy: 0.1617\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 398s 3s/step - loss: 1.6401 - accuracy: 0.2763 - val_loss: 33.0710 - val_accuracy: 0.1617\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 385s 3s/step - loss: 1.6097 - accuracy: 0.3117 - val_loss: 16.1516 - val_accuracy: 0.2108\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 383s 3s/step - loss: 1.5202 - accuracy: 0.3581 - val_loss: 9.0943 - val_accuracy: 0.3150\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 384s 3s/step - loss: 1.4684 - accuracy: 0.3827 - val_loss: 5.7529 - val_accuracy: 0.3667\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 1411s 9s/step - loss: 1.4448 - accuracy: 0.4054 - val_loss: 4.7067 - val_accuracy: 0.4183\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 441s 3s/step - loss: 1.3773 - accuracy: 0.4344 - val_loss: 3.4485 - val_accuracy: 0.4417\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 428s 3s/step - loss: 1.3310 - accuracy: 0.4606 - val_loss: 2.8178 - val_accuracy: 0.4558\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 427s 3s/step - loss: 1.3019 - accuracy: 0.4754 - val_loss: 3.1805 - val_accuracy: 0.4483\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 419s 3s/step - loss: 1.2252 - accuracy: 0.5046 - val_loss: 2.1970 - val_accuracy: 0.5308\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 9799s 66s/step - loss: 1.1729 - accuracy: 0.5075 - val_loss: 2.3356 - val_accuracy: 0.5025\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 398s 3s/step - loss: 1.1413 - accuracy: 0.5333 - val_loss: 2.8344 - val_accuracy: 0.4950\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 414s 3s/step - loss: 1.1151 - accuracy: 0.5417 - val_loss: 2.5230 - val_accuracy: 0.5258\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 424s 3s/step - loss: 1.0611 - accuracy: 0.5575 - val_loss: 2.0823 - val_accuracy: 0.5383\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 422s 3s/step - loss: 1.0138 - accuracy: 0.5740 - val_loss: 2.5513 - val_accuracy: 0.4958\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 4336s 29s/step - loss: 0.9904 - accuracy: 0.5923 - val_loss: 2.4163 - val_accuracy: 0.5200\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 428s 3s/step - loss: 0.9478 - accuracy: 0.6071 - val_loss: 2.5517 - val_accuracy: 0.5100\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 427s 3s/step - loss: 0.9191 - accuracy: 0.6123 - val_loss: 2.7031 - val_accuracy: 0.5017\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 418s 3s/step - loss: 0.8829 - accuracy: 0.6354 - val_loss: 2.1953 - val_accuracy: 0.5617\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 5072s 34s/step - loss: 0.8695 - accuracy: 0.6419 - val_loss: 2.9212 - val_accuracy: 0.5083\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 394s 3s/step - loss: 0.8355 - accuracy: 0.6612 - val_loss: 3.7672 - val_accuracy: 0.4875\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 411s 3s/step - loss: 0.7967 - accuracy: 0.6750 - val_loss: 2.2375 - val_accuracy: 0.5583\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 418s 3s/step - loss: 0.7742 - accuracy: 0.6790 - val_loss: 2.7291 - val_accuracy: 0.5408\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 421s 3s/step - loss: 0.7338 - accuracy: 0.6929 - val_loss: 1.9189 - val_accuracy: 0.6125\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 690s 5s/step - loss: 0.7168 - accuracy: 0.7088 - val_loss: 4.2867 - val_accuracy: 0.4808\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 527s 4s/step - loss: 0.6621 - accuracy: 0.7258 - val_loss: 2.3347 - val_accuracy: 0.5967\n",
      "Epoch 27/50\n",
      "150/150 [==============================] - 500s 3s/step - loss: 0.6392 - accuracy: 0.7390 - val_loss: 2.3127 - val_accuracy: 0.5975\n",
      "Epoch 28/50\n",
      "150/150 [==============================] - 495s 3s/step - loss: 0.6163 - accuracy: 0.7535 - val_loss: 2.7557 - val_accuracy: 0.6083\n",
      "Epoch 29/50\n",
      "150/150 [==============================] - 497s 3s/step - loss: 0.5912 - accuracy: 0.7550 - val_loss: 2.6633 - val_accuracy: 0.6017\n",
      "Epoch 30/50\n",
      "150/150 [==============================] - 499s 3s/step - loss: 0.5810 - accuracy: 0.7525 - val_loss: 4.3328 - val_accuracy: 0.5317\n",
      "Epoch 31/50\n",
      "150/150 [==============================] - 489s 3s/step - loss: 0.5482 - accuracy: 0.7702 - val_loss: 2.8736 - val_accuracy: 0.6025\n",
      "Epoch 32/50\n",
      "150/150 [==============================] - 495s 3s/step - loss: 0.5377 - accuracy: 0.7763 - val_loss: 2.3798 - val_accuracy: 0.6158\n",
      "Epoch 33/50\n",
      "150/150 [==============================] - 514s 3s/step - loss: 0.5301 - accuracy: 0.7885 - val_loss: 2.5380 - val_accuracy: 0.6242\n",
      "Epoch 34/50\n",
      "150/150 [==============================] - 495s 3s/step - loss: 0.4907 - accuracy: 0.7948 - val_loss: 5.4632 - val_accuracy: 0.5375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1be0603e170>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 이미지 크기 설정\n",
    "img_size = (224, 224) # 이미지 크키가 크면 메모리를 많이 차지해서 설정 필요\n",
    "\n",
    "# 데이터 로드 및 전처리 함수\n",
    "def load_and_preprocess_data(image_paths, label, brightness=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 랜덤하게 100장만 선택\n",
    "    selected_images = random.sample(image_paths, 1000) #1000개 이미지\n",
    "\n",
    "    for img_path in selected_images:\n",
    "        img = load_img(img_path, target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "\n",
    "        # 이미지 명도와 채도 높이기\n",
    "        if brightness:\n",
    "            img_array = cv2.convertScaleAbs(img_array, alpha=1.2, beta=30)\n",
    "\n",
    "        img_array = img_array.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n",
    "\n",
    "        data.append(img_array)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "# 카테고리 리스트\n",
    "categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# 이미지 데이터 로드 및 전처리\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_path = os.path.join(\"C:\\\\Users\\\\김현\\\\Final_Project\\\\crawled_img\", category) # crawled_img/안에 있는 카테고리 리스트\n",
    "    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n",
    "    \n",
    "    data, labels = load_and_preprocess_data(category_images, label=i)\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# 데이터 합치기 및 레이블 변환\n",
    "X = np.array(all_data)\n",
    "y = to_categorical(np.array(all_labels), num_classes=num_classes)\n",
    "\n",
    "# 학습 및 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout 추가\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# EarlyStopping 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9ffdc4d-b5bb-4950-a64b-7acafbb4b379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 21s 556ms/step - loss: 5.4632 - accuracy: 0.5375\n",
      "테스트 손실: 5.4632\n",
      "테스트 정확도: 53.75%\n"
     ]
    }
   ],
   "source": [
    "# 테스트 세트에서 모델 평가\n",
    "evaluation_results = model.evaluate(X_test, y_test)\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(f'테스트 손실: {evaluation_results[0]:.4f}')\n",
    "print(f'테스트 정확도: {evaluation_results[1]*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d3f03bc-748c-4339-abc0-d9b87099920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "# 전체 모델 저장\n",
    "model.save(\"C:/Users/김현/Final_Project/epoch50_up.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451a441-322f-4a3f-befe-8f3bfdd963d1",
   "metadata": {},
   "source": [
    "# epoch_up에다가 데이터 증강 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ceb9feb-6f73-4635-af0f-89ef4eda13d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "150/150 [==============================] - 608s 4s/step - loss: 1.9026 - accuracy: 0.2515 - val_loss: 27.9914 - val_accuracy: 0.1617\n",
      "Epoch 2/50\n",
      "150/150 [==============================] - 505s 3s/step - loss: 1.6750 - accuracy: 0.2894 - val_loss: 22.4360 - val_accuracy: 0.1642\n",
      "Epoch 3/50\n",
      "150/150 [==============================] - 460s 3s/step - loss: 1.6630 - accuracy: 0.2923 - val_loss: 10.3830 - val_accuracy: 0.2400\n",
      "Epoch 4/50\n",
      "150/150 [==============================] - 459s 3s/step - loss: 1.6528 - accuracy: 0.3081 - val_loss: 4.1308 - val_accuracy: 0.3283\n",
      "Epoch 5/50\n",
      "150/150 [==============================] - 450s 3s/step - loss: 1.6315 - accuracy: 0.2988 - val_loss: 3.1668 - val_accuracy: 0.3108\n",
      "Epoch 6/50\n",
      "150/150 [==============================] - 446s 3s/step - loss: 1.6227 - accuracy: 0.3090 - val_loss: 2.5991 - val_accuracy: 0.3025\n",
      "Epoch 7/50\n",
      "150/150 [==============================] - 468s 3s/step - loss: 1.5917 - accuracy: 0.3133 - val_loss: 2.3374 - val_accuracy: 0.2983\n",
      "Epoch 8/50\n",
      "150/150 [==============================] - 420s 3s/step - loss: 1.5896 - accuracy: 0.3113 - val_loss: 2.7163 - val_accuracy: 0.2975\n",
      "Epoch 9/50\n",
      "150/150 [==============================] - 467s 3s/step - loss: 1.5908 - accuracy: 0.3144 - val_loss: 2.3441 - val_accuracy: 0.3250\n",
      "Epoch 10/50\n",
      "150/150 [==============================] - 432s 3s/step - loss: 1.5681 - accuracy: 0.3298 - val_loss: 2.5606 - val_accuracy: 0.3092\n",
      "Epoch 11/50\n",
      "150/150 [==============================] - 433s 3s/step - loss: 1.5486 - accuracy: 0.3338 - val_loss: 2.3851 - val_accuracy: 0.3117\n",
      "Epoch 12/50\n",
      "150/150 [==============================] - 543s 4s/step - loss: 1.5504 - accuracy: 0.3225 - val_loss: 2.1319 - val_accuracy: 0.3333\n",
      "Epoch 13/50\n",
      "150/150 [==============================] - 555s 4s/step - loss: 1.5430 - accuracy: 0.3469 - val_loss: 3.0442 - val_accuracy: 0.3367\n",
      "Epoch 14/50\n",
      "150/150 [==============================] - 532s 4s/step - loss: 1.5193 - accuracy: 0.3487 - val_loss: 3.3938 - val_accuracy: 0.3100\n",
      "Epoch 15/50\n",
      "150/150 [==============================] - 579s 4s/step - loss: 1.5240 - accuracy: 0.3502 - val_loss: 4.9849 - val_accuracy: 0.3042\n",
      "Epoch 16/50\n",
      "150/150 [==============================] - 520s 3s/step - loss: 1.5093 - accuracy: 0.3560 - val_loss: 3.8389 - val_accuracy: 0.3383\n",
      "Epoch 17/50\n",
      "150/150 [==============================] - 573s 4s/step - loss: 1.5042 - accuracy: 0.3683 - val_loss: 3.0984 - val_accuracy: 0.3483\n",
      "Epoch 18/50\n",
      "150/150 [==============================] - 517s 3s/step - loss: 1.4813 - accuracy: 0.3885 - val_loss: 2.0732 - val_accuracy: 0.3925\n",
      "Epoch 19/50\n",
      "150/150 [==============================] - 455s 3s/step - loss: 1.4815 - accuracy: 0.3877 - val_loss: 2.8367 - val_accuracy: 0.3692\n",
      "Epoch 20/50\n",
      "150/150 [==============================] - 581s 4s/step - loss: 1.4593 - accuracy: 0.3994 - val_loss: 4.9758 - val_accuracy: 0.3308\n",
      "Epoch 21/50\n",
      "150/150 [==============================] - 521s 3s/step - loss: 1.4509 - accuracy: 0.3927 - val_loss: 3.3855 - val_accuracy: 0.3733\n",
      "Epoch 22/50\n",
      "150/150 [==============================] - 572s 4s/step - loss: 1.4170 - accuracy: 0.4102 - val_loss: 2.9075 - val_accuracy: 0.4000\n",
      "Epoch 23/50\n",
      "150/150 [==============================] - 477s 3s/step - loss: 1.4520 - accuracy: 0.3996 - val_loss: 2.8731 - val_accuracy: 0.3817\n",
      "Epoch 24/50\n",
      "150/150 [==============================] - 540s 4s/step - loss: 1.4416 - accuracy: 0.4073 - val_loss: 2.0442 - val_accuracy: 0.4408\n",
      "Epoch 25/50\n",
      "150/150 [==============================] - 625s 4s/step - loss: 1.4096 - accuracy: 0.4265 - val_loss: 6.1562 - val_accuracy: 0.3217\n",
      "Epoch 26/50\n",
      "150/150 [==============================] - 650s 4s/step - loss: 1.4009 - accuracy: 0.4181 - val_loss: 5.4920 - val_accuracy: 0.3308\n",
      "Epoch 27/50\n",
      "150/150 [==============================] - 762s 5s/step - loss: 1.3862 - accuracy: 0.4175 - val_loss: 3.0072 - val_accuracy: 0.4000\n",
      "Epoch 28/50\n",
      "150/150 [==============================] - 1012s 7s/step - loss: 1.3915 - accuracy: 0.4198 - val_loss: 4.4284 - val_accuracy: 0.3992\n",
      "Epoch 29/50\n",
      "150/150 [==============================] - 989s 7s/step - loss: 1.3891 - accuracy: 0.4281 - val_loss: 3.6710 - val_accuracy: 0.4083\n",
      "Epoch 30/50\n",
      "150/150 [==============================] - 1036s 7s/step - loss: 1.3408 - accuracy: 0.4298 - val_loss: 4.1436 - val_accuracy: 0.3642\n",
      "Epoch 31/50\n",
      "150/150 [==============================] - 799s 5s/step - loss: 1.3590 - accuracy: 0.4421 - val_loss: 3.7548 - val_accuracy: 0.3967\n",
      "Epoch 32/50\n",
      "150/150 [==============================] - 713s 5s/step - loss: 1.3475 - accuracy: 0.4342 - val_loss: 3.4887 - val_accuracy: 0.4192\n",
      "Epoch 33/50\n",
      "150/150 [==============================] - 823s 5s/step - loss: 1.3362 - accuracy: 0.4442 - val_loss: 2.0263 - val_accuracy: 0.5450\n",
      "Epoch 34/50\n",
      "150/150 [==============================] - 686s 5s/step - loss: 1.3279 - accuracy: 0.4613 - val_loss: 2.7077 - val_accuracy: 0.4442\n",
      "Epoch 35/50\n",
      "150/150 [==============================] - 644s 4s/step - loss: 1.3183 - accuracy: 0.4490 - val_loss: 3.8906 - val_accuracy: 0.4275\n",
      "Epoch 36/50\n",
      "150/150 [==============================] - 696s 5s/step - loss: 1.3117 - accuracy: 0.4598 - val_loss: 3.8511 - val_accuracy: 0.4125\n",
      "Epoch 37/50\n",
      "150/150 [==============================] - 775s 5s/step - loss: 1.2876 - accuracy: 0.4712 - val_loss: 6.8550 - val_accuracy: 0.3533\n",
      "Epoch 38/50\n",
      "150/150 [==============================] - 513s 3s/step - loss: 1.2906 - accuracy: 0.4725 - val_loss: 4.6953 - val_accuracy: 0.3958\n",
      "Epoch 39/50\n",
      "150/150 [==============================] - 717s 5s/step - loss: 1.2948 - accuracy: 0.4733 - val_loss: 1.9491 - val_accuracy: 0.5608\n",
      "Epoch 40/50\n",
      "150/150 [==============================] - 676s 5s/step - loss: 1.2917 - accuracy: 0.4688 - val_loss: 10.5105 - val_accuracy: 0.3125\n",
      "Epoch 41/50\n",
      "150/150 [==============================] - 697s 5s/step - loss: 1.2867 - accuracy: 0.4783 - val_loss: 7.0522 - val_accuracy: 0.3575\n",
      "Epoch 42/50\n",
      "150/150 [==============================] - 499s 3s/step - loss: 1.2642 - accuracy: 0.4915 - val_loss: 3.6251 - val_accuracy: 0.4617\n",
      "Epoch 43/50\n",
      "150/150 [==============================] - 472s 3s/step - loss: 1.2554 - accuracy: 0.4979 - val_loss: 9.5398 - val_accuracy: 0.3433\n",
      "Epoch 44/50\n",
      "150/150 [==============================] - 471s 3s/step - loss: 1.2252 - accuracy: 0.5035 - val_loss: 7.5184 - val_accuracy: 0.3625\n",
      "Epoch 45/50\n",
      "150/150 [==============================] - 464s 3s/step - loss: 1.2139 - accuracy: 0.5196 - val_loss: 7.9553 - val_accuracy: 0.3650\n",
      "Epoch 46/50\n",
      "150/150 [==============================] - 957s 6s/step - loss: 1.2428 - accuracy: 0.5079 - val_loss: 13.3540 - val_accuracy: 0.3392\n",
      "Epoch 47/50\n",
      "150/150 [==============================] - 538s 4s/step - loss: 1.2090 - accuracy: 0.5104 - val_loss: 6.3612 - val_accuracy: 0.4167\n",
      "Epoch 48/50\n",
      "150/150 [==============================] - 542s 4s/step - loss: 1.2018 - accuracy: 0.5125 - val_loss: 13.4963 - val_accuracy: 0.3208\n",
      "Epoch 49/50\n",
      "150/150 [==============================] - 548s 4s/step - loss: 1.1867 - accuracy: 0.5121 - val_loss: 14.4922 - val_accuracy: 0.3225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f567e9e1a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 이미지 크기 설정\n",
    "img_size = (224, 224) # 이미지 크키가 크면 메모리를 많이 차지해서 설정 필요\n",
    "\n",
    "# 데이터 로드 및 전처리 함수\n",
    "def load_and_preprocess_data(image_paths, label, brightness=True):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # 랜덤하게 100장만 선택\n",
    "    selected_images = random.sample(image_paths, 1000) #1000개 이미지\n",
    "\n",
    "    for img_path in selected_images:\n",
    "        img = load_img(img_path, target_size=img_size)\n",
    "        img_array = img_to_array(img)\n",
    "\n",
    "        # 이미지 명도와 채도 높이기\n",
    "        if brightness:\n",
    "            img_array = cv2.convertScaleAbs(img_array, alpha=1.2, beta=30)\n",
    "\n",
    "        img_array = img_array.astype('float32') / 255.0  # 이미지를 0과 1 사이 값으로 정규화\n",
    "\n",
    "        data.append(img_array)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "# 카테고리 리스트\n",
    "categories = [\"joyful\", \"adventure\", \"tradition\", \"nature\", \"cultural\", \"art\"]\n",
    "num_classes = len(categories)\n",
    "\n",
    "# 이미지 데이터 로드 및 전처리\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_path = os.path.join(\"C:\\\\Users\\\\김현\\\\Final_Project\\\\crawled_img\", category) # crawled_img/안에 있는 카테고리 리스트\n",
    "    category_images = [os.path.join(category_path, img) for img in os.listdir(category_path)]\n",
    "    \n",
    "    data, labels = load_and_preprocess_data(category_images, label=i)\n",
    "    all_data.extend(data)\n",
    "    all_labels.extend(labels)\n",
    "\n",
    "# 데이터 합치기 및 레이블 변환\n",
    "X = np.array(all_data)\n",
    "y = to_categorical(np.array(all_labels), num_classes=num_classes)\n",
    "\n",
    "# 학습 및 테스트 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.25))  # Dropout 추가\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout 추가\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 데이터 증강(Data Augmentation) 설정\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,        # 무작위 회전 범위\n",
    "    width_shift_range=0.2,    # 무작위 수평 이동 범위\n",
    "    height_shift_range=0.2,   # 무작위 수직 이동 범위\n",
    "    horizontal_flip=True)     # 무작위 수평 뒤집기\n",
    "\n",
    "# Data Augmentation 적용\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# EarlyStopping 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=32), \n",
    "          steps_per_epoch=len(X_train) / 32, \n",
    "          epochs=50, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eebd841-d834-456a-a816-645f1d9d8bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 24s 642ms/step - loss: 14.4922 - accuracy: 0.3225\n",
      "테스트 손실: 14.4922\n",
      "테스트 정확도: 32.25%\n"
     ]
    }
   ],
   "source": [
    "# 테스트 세트에서 모델 평가\n",
    "evaluation_results = model.evaluate(X_test, y_test)\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(f'테스트 손실: {evaluation_results[0]:.4f}')\n",
    "print(f'테스트 정확도: {evaluation_results[1]*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd550d7-47a8-4188-aa85-6ac3e3612639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\김현\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import save_model, load_model\n",
    "\n",
    "# 전체 모델 저장\n",
    "model.save(\"C:/Users/김현/Final_Project/epoch50_up_data.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
